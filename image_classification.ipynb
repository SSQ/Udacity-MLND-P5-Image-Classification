{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions.\n",
    "\n",
    "1. 0-9\n",
    "1. 0-255\n",
    "1. random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 4:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1003, 1: 963, 2: 1041, 3: 976, 4: 1004, 5: 1021, 6: 1004, 7: 981, 8: 1024, 9: 983}\n",
      "First 20 Labels: [0, 6, 0, 2, 7, 2, 1, 2, 4, 1, 5, 6, 6, 3, 1, 3, 5, 5, 8, 1]\n",
      "\n",
      "Example of Image 15:\n",
      "Image - Min Value: 0 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 3 Name: cat\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrYYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc380978>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 4\n",
    "sample_id = 15\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x =x*1.0/255\n",
    "    x = np.array(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Look into LabelBinarizer in the preprocessing module of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    one_hot_labels = np.zeros((len(x),10))\n",
    "    for idx, val in enumerate(x):\n",
    "        one_hot_labels[idx,val] = 1\n",
    "        \n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None,image_shape[0],image_shape[1],image_shape[2]], name=\"x\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    y = tf.placeholder(tf.float32, shape=[None,n_classes], name=\"y\")\n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    return keep_prob\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers.\n",
    "\n",
    "** Hint: **\n",
    "\n",
    "When unpacking values as an argument in Python, look into the [unpacking](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists) operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(x_tensor.shape[1]) # 32\n",
    "    #print(conv_ksize) # (2, 2)\n",
    "    #print(conv_num_outputs) # 10\n",
    "    #print(type(conv_num_outputs))\n",
    "    #print((x_tensor))\n",
    "    #print(type(x_tensor.shape[3]))\n",
    "    #print(x_tensor.shape) # (?, 32, 32, 5)\n",
    "\n",
    "    #print(type(x_tensor.shape)) # <class 'tensorflow.python.framework.tensor_shape.TensorShape'>\n",
    "    #print(type(tf.to_int32(x_tensor.shape[3])))\n",
    "    #print(type(conv_num_outputs))\n",
    "    #print(type(x_tensor)) # <class 'tensorflow.python.framework.ops.Tensor'>    \n",
    "    #print(type(conv_strides[0]))\n",
    "        \n",
    "    weight = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1],tf.to_int32(x_tensor.shape[3]),conv_num_outputs])) # (height, width, input_depth, output_depth)\n",
    "    #print(type(weight)) # <class 'tensorflow.python.ops.variables.Variable'>\n",
    "    bias = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "\n",
    "    conv_layer = tf.nn.conv2d(x_tensor,weight, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    # Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    # Apply activation function\n",
    "    conv_layer = tf.nn.relu(conv_layer)                \n",
    "    # Apply Max Pooling\n",
    "    conv_layer = tf.nn.max_pool(\n",
    "        conv_layer,\n",
    "        ksize=[1, pool_ksize[0], pool_ksize[1], 1],\n",
    "        strides=[1, pool_strides[0], pool_strides[1], 1],\n",
    "        padding='SAME')\n",
    "\n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Flatten input from: [None, height, width, channels]\n",
    "    # To: [None, height * width * channels] \n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs,tf.nn.relu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs,None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    x = conv2d_maxpool(x, 90, (3,3), (1,1), (2,2), (2,2))\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    x = flatten(x)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    x= fully_conn(x, 500)\n",
    "    x=tf.nn.dropout(x, keep_prob)\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    x=output(x,10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: keep_probability})\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Calculate batch loss and accuracy\n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.})\n",
    "    validation_accuracy  = session.run(accuracy, feed_dict={\n",
    "        x: valid_features,\n",
    "        y: valid_labels,\n",
    "        keep_prob: 1.})\n",
    "    print('loss: {}, validation_accuracy: {}'.format(loss, validation_accuracy))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.286597728729248, validation_accuracy: 0.16660000383853912\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 2.2608461380004883, validation_accuracy: 0.1876000016927719\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 2.233574390411377, validation_accuracy: 0.18860000371932983\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 2.219876766204834, validation_accuracy: 0.18559999763965607\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 2.169370174407959, validation_accuracy: 0.21080000698566437\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 2.1597862243652344, validation_accuracy: 0.24060000479221344\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 2.0926074981689453, validation_accuracy: 0.2433999925851822\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 2.074634075164795, validation_accuracy: 0.2824000120162964\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 2.0437264442443848, validation_accuracy: 0.2565999925136566\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 1.9920070171356201, validation_accuracy: 0.2378000020980835\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 1.9396941661834717, validation_accuracy: 0.3089999854564667\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 1.9256346225738525, validation_accuracy: 0.2906000018119812\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 1.8934745788574219, validation_accuracy: 0.33640000224113464\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 1.852295160293579, validation_accuracy: 0.3156000077724457\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 1.8598839044570923, validation_accuracy: 0.33000001311302185\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 1.6960747241973877, validation_accuracy: 0.373199999332428\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 1.6690263748168945, validation_accuracy: 0.3637999892234802\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 1.5939161777496338, validation_accuracy: 0.41359999775886536\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 1.580227017402649, validation_accuracy: 0.38359999656677246\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 1.6957870721817017, validation_accuracy: 0.39579999446868896\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 1.5709902048110962, validation_accuracy: 0.37540000677108765\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 1.602400541305542, validation_accuracy: 0.376800000667572\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 1.6383559703826904, validation_accuracy: 0.38999998569488525\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 1.605372667312622, validation_accuracy: 0.38179999589920044\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 1.5128133296966553, validation_accuracy: 0.4075999855995178\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 1.5238693952560425, validation_accuracy: 0.4075999855995178\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 1.4909723997116089, validation_accuracy: 0.3824000060558319\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 1.4644263982772827, validation_accuracy: 0.4235999882221222\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 1.4931217432022095, validation_accuracy: 0.3792000114917755\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 1.4267784357070923, validation_accuracy: 0.41200000047683716\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 1.435096025466919, validation_accuracy: 0.39660000801086426\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 1.4634679555892944, validation_accuracy: 0.36559998989105225\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 1.407008171081543, validation_accuracy: 0.3928000032901764\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 1.3900117874145508, validation_accuracy: 0.4059999883174896\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 1.4326642751693726, validation_accuracy: 0.391400009393692\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 1.513201355934143, validation_accuracy: 0.3617999851703644\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 1.3596738576889038, validation_accuracy: 0.4041999876499176\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 1.4151428937911987, validation_accuracy: 0.40700000524520874\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 1.4102497100830078, validation_accuracy: 0.4115999937057495\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 1.3512499332427979, validation_accuracy: 0.41019999980926514\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 1.4175455570220947, validation_accuracy: 0.3986000120639801\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 1.4789068698883057, validation_accuracy: 0.38519999384880066\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 1.3690767288208008, validation_accuracy: 0.41819998621940613\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 1.4669851064682007, validation_accuracy: 0.3646000027656555\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 1.3087522983551025, validation_accuracy: 0.428600013256073\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 1.1820662021636963, validation_accuracy: 0.4447999894618988\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 1.2393877506256104, validation_accuracy: 0.39239999651908875\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 1.2630723714828491, validation_accuracy: 0.4246000051498413\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 1.2347853183746338, validation_accuracy: 0.42879998683929443\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 1.2892332077026367, validation_accuracy: 0.40619999170303345\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 1.220494031906128, validation_accuracy: 0.42719998955726624\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 1.1973060369491577, validation_accuracy: 0.4537999927997589\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 1.111426591873169, validation_accuracy: 0.45239999890327454\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 1.1599171161651611, validation_accuracy: 0.43880000710487366\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 1.1175950765609741, validation_accuracy: 0.44279998540878296\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 1.1253629922866821, validation_accuracy: 0.4480000138282776\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 1.1250032186508179, validation_accuracy: 0.41940000653266907\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 1.1245731115341187, validation_accuracy: 0.43639999628067017\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 1.2235057353973389, validation_accuracy: 0.41920000314712524\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 1.1982924938201904, validation_accuracy: 0.41819998621940613\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 1.1312816143035889, validation_accuracy: 0.43160000443458557\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 1.1593549251556396, validation_accuracy: 0.420199990272522\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 1.0893160104751587, validation_accuracy: 0.45339998602867126\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 1.1755180358886719, validation_accuracy: 0.43779999017715454\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 1.124039649963379, validation_accuracy: 0.43299999833106995\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 1.1064244508743286, validation_accuracy: 0.45019999146461487\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 1.111121654510498, validation_accuracy: 0.4381999969482422\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 1.142662763595581, validation_accuracy: 0.4058000147342682\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 1.09291410446167, validation_accuracy: 0.43479999899864197\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 1.1072876453399658, validation_accuracy: 0.42559999227523804\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 1.0509974956512451, validation_accuracy: 0.44279998540878296\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 1.0497888326644897, validation_accuracy: 0.44339999556541443\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 1.110837459564209, validation_accuracy: 0.4212000072002411\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 1.0436789989471436, validation_accuracy: 0.43779999017715454\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 1.0488715171813965, validation_accuracy: 0.42500001192092896\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 1.0386496782302856, validation_accuracy: 0.4336000084877014\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 1.0366649627685547, validation_accuracy: 0.43779999017715454\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 1.0736415386199951, validation_accuracy: 0.4253999888896942\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 1.0221624374389648, validation_accuracy: 0.4498000144958496\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 1.0411878824234009, validation_accuracy: 0.43380001187324524\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 1.0945693254470825, validation_accuracy: 0.39820000529289246\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 1.0235023498535156, validation_accuracy: 0.4221999943256378\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 1.005242109298706, validation_accuracy: 0.45019999146461487\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 1.0086652040481567, validation_accuracy: 0.4449999928474426\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 1.0258127450942993, validation_accuracy: 0.42719998955726624\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 1.0003936290740967, validation_accuracy: 0.4431999921798706\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 0.9974465370178223, validation_accuracy: 0.4406000077724457\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 0.9690775871276855, validation_accuracy: 0.4503999948501587\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 0.9516843557357788, validation_accuracy: 0.4641999900341034\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 0.9449468851089478, validation_accuracy: 0.46560001373291016\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 0.9379709362983704, validation_accuracy: 0.45579999685287476\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 0.9612709283828735, validation_accuracy: 0.446399986743927\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 0.9628978967666626, validation_accuracy: 0.4440000057220459\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 0.9345110058784485, validation_accuracy: 0.4519999921321869\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 0.9183340072631836, validation_accuracy: 0.46299999952316284\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 0.9288328289985657, validation_accuracy: 0.4618000090122223\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 0.92071133852005, validation_accuracy: 0.45840001106262207\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 0.9031446576118469, validation_accuracy: 0.4602000117301941\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 0.8779818415641785, validation_accuracy: 0.4575999975204468\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 0.8448753356933594, validation_accuracy: 0.4733999967575073\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss: 0.8524448275566101, validation_accuracy: 0.4668000042438507\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss: 0.8487033843994141, validation_accuracy: 0.4726000130176544\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss: 0.8428546786308289, validation_accuracy: 0.46399998664855957\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss: 0.8211138844490051, validation_accuracy: 0.46860000491142273\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss: 0.807076632976532, validation_accuracy: 0.47760000824928284\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss: 0.7910637259483337, validation_accuracy: 0.46219998598098755\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss: 0.7686703205108643, validation_accuracy: 0.4745999872684479\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss: 0.7313634157180786, validation_accuracy: 0.46880000829696655\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss: 0.7442789077758789, validation_accuracy: 0.4860000014305115\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss: 0.7323124408721924, validation_accuracy: 0.4867999851703644\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss: 0.6957942843437195, validation_accuracy: 0.48739999532699585\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss: 0.7043094635009766, validation_accuracy: 0.47839999198913574\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss: 0.698219895362854, validation_accuracy: 0.48820000886917114\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss: 0.6757155656814575, validation_accuracy: 0.49239999055862427\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss: 0.6735906600952148, validation_accuracy: 0.4957999885082245\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss: 0.6943930387496948, validation_accuracy: 0.4837999939918518\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss: 0.6545528769493103, validation_accuracy: 0.4844000041484833\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss: 0.6862460374832153, validation_accuracy: 0.47360000014305115\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss: 0.6597196459770203, validation_accuracy: 0.4729999899864197\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss: 0.6870065331459045, validation_accuracy: 0.47679999470710754\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss: 0.6719897389411926, validation_accuracy: 0.4950000047683716\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss: 0.6625443696975708, validation_accuracy: 0.49540001153945923\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss: 0.7038499712944031, validation_accuracy: 0.48820000886917114\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss: 0.67188560962677, validation_accuracy: 0.4925999939441681\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss: 0.6422711610794067, validation_accuracy: 0.4851999878883362\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss: 0.6524574160575867, validation_accuracy: 0.48339998722076416\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss: 0.6415354609489441, validation_accuracy: 0.4941999912261963\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss: 0.6093233823776245, validation_accuracy: 0.49880000948905945\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss: 0.6289370059967041, validation_accuracy: 0.49059998989105225\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss: 0.6245549321174622, validation_accuracy: 0.508400022983551\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss: 0.5871565341949463, validation_accuracy: 0.4984000027179718\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss: 0.6201786994934082, validation_accuracy: 0.5040000081062317\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss: 0.6036598086357117, validation_accuracy: 0.5117999911308289\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss: 0.5833679437637329, validation_accuracy: 0.5135999917984009\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss: 0.5589113235473633, validation_accuracy: 0.5088000297546387\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss: 0.5301605463027954, validation_accuracy: 0.5163999795913696\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss: 0.5461441278457642, validation_accuracy: 0.5230000019073486\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss: 0.5171568393707275, validation_accuracy: 0.5171999931335449\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss: 0.5214236378669739, validation_accuracy: 0.5121999979019165\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss: 0.5198658108711243, validation_accuracy: 0.5167999863624573\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss: 0.4866858124732971, validation_accuracy: 0.5231999754905701\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss: 0.5059729814529419, validation_accuracy: 0.522599995136261\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss: 0.5023648142814636, validation_accuracy: 0.5206000208854675\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss: 0.49492859840393066, validation_accuracy: 0.5284000039100647\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss: 0.4701051115989685, validation_accuracy: 0.5230000019073486\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss: 0.48220086097717285, validation_accuracy: 0.5058000087738037\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss: 0.4618465006351471, validation_accuracy: 0.5238000154495239\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss: 0.43808022141456604, validation_accuracy: 0.5156000256538391\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss: 0.46903032064437866, validation_accuracy: 0.5231999754905701\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss: 0.448292076587677, validation_accuracy: 0.5288000106811523\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss: 0.4379674792289734, validation_accuracy: 0.5203999876976013\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss: 0.44969385862350464, validation_accuracy: 0.5103999972343445\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss: 0.4553235173225403, validation_accuracy: 0.5206000208854675\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss: 0.4381825923919678, validation_accuracy: 0.5156000256538391\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss: 0.44751691818237305, validation_accuracy: 0.5149999856948853\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss: 0.42751169204711914, validation_accuracy: 0.5163999795913696\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss: 0.4449084401130676, validation_accuracy: 0.5109999775886536\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss: 0.42932599782943726, validation_accuracy: 0.5105999708175659\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss: 0.4379563331604004, validation_accuracy: 0.5088000297546387\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss: 0.42892393469810486, validation_accuracy: 0.5103999972343445\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss: 0.45078134536743164, validation_accuracy: 0.5135999917984009\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss: 0.4398556351661682, validation_accuracy: 0.5126000046730042\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss: 0.44477328658103943, validation_accuracy: 0.5185999870300293\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss: 0.42325812578201294, validation_accuracy: 0.5116000175476074\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss: 0.4191775321960449, validation_accuracy: 0.517799973487854\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss: 0.4369875490665436, validation_accuracy: 0.5090000033378601\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss: 0.40439414978027344, validation_accuracy: 0.5099999904632568\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss: 0.417550653219223, validation_accuracy: 0.49959999322891235\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss: 0.391564279794693, validation_accuracy: 0.5070000290870667\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss: 0.39201295375823975, validation_accuracy: 0.49900001287460327\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss: 0.39546114206314087, validation_accuracy: 0.49399998784065247\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss: 0.40108951926231384, validation_accuracy: 0.49239999055862427\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss: 0.416148841381073, validation_accuracy: 0.5148000121116638\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss: 0.40132513642311096, validation_accuracy: 0.5070000290870667\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss: 0.4127449095249176, validation_accuracy: 0.5135999917984009\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss: 0.4163587689399719, validation_accuracy: 0.5170000195503235\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss: 0.40334194898605347, validation_accuracy: 0.5127999782562256\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss: 0.3884821832180023, validation_accuracy: 0.520799994468689\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss: 0.3906897306442261, validation_accuracy: 0.5180000066757202\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss: 0.3707887530326843, validation_accuracy: 0.515999972820282\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss: 0.38255611062049866, validation_accuracy: 0.5144000053405762\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss: 0.3727361559867859, validation_accuracy: 0.5217999815940857\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss: 0.3693450093269348, validation_accuracy: 0.5216000080108643\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss: 0.3582543730735779, validation_accuracy: 0.5090000033378601\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss: 0.33015257120132446, validation_accuracy: 0.5206000208854675\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss: 0.34633898735046387, validation_accuracy: 0.5121999979019165\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss: 0.3862294554710388, validation_accuracy: 0.5116000175476074\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss: 0.361930787563324, validation_accuracy: 0.5171999931335449\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss: 0.34545284509658813, validation_accuracy: 0.5266000032424927\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss: 0.34360677003860474, validation_accuracy: 0.522599995136261\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss: 0.32132014632225037, validation_accuracy: 0.5342000126838684\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss: 0.3264082372188568, validation_accuracy: 0.5303999781608582\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss: 0.31287845969200134, validation_accuracy: 0.5199999809265137\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss: 0.30184945464134216, validation_accuracy: 0.5098000168800354\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss: 0.3257822096347809, validation_accuracy: 0.5181999802589417\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss: 0.3071195185184479, validation_accuracy: 0.5144000053405762\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss: 0.3666459023952484, validation_accuracy: 0.5055999755859375\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss: 0.3457721769809723, validation_accuracy: 0.5120000243186951\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss: 0.3101114332675934, validation_accuracy: 0.5126000046730042\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss: 0.2977718412876129, validation_accuracy: 0.503000020980835\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.284106492996216, validation_accuracy: 0.13279999792575836\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss: 2.2302663326263428, validation_accuracy: 0.17139999568462372\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss: 2.1874442100524902, validation_accuracy: 0.21559999883174896\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss: 2.110159397125244, validation_accuracy: 0.20960000157356262\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss: 2.11669921875, validation_accuracy: 0.22519999742507935\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 2.149503231048584, validation_accuracy: 0.24120000004768372\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss: 2.0570878982543945, validation_accuracy: 0.25200000405311584\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss: 2.001974582672119, validation_accuracy: 0.272599995136261\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss: 1.9950520992279053, validation_accuracy: 0.26019999384880066\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss: 2.0598506927490234, validation_accuracy: 0.21899999678134918\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 2.055422306060791, validation_accuracy: 0.27140000462532043\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss: 1.9023144245147705, validation_accuracy: 0.2752000093460083\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss: 1.720282793045044, validation_accuracy: 0.27219998836517334\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss: 1.8355376720428467, validation_accuracy: 0.2741999924182892\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss: 1.728580117225647, validation_accuracy: 0.27619999647140503\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 1.9715343713760376, validation_accuracy: 0.2797999978065491\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss: 1.7881958484649658, validation_accuracy: 0.27959999442100525\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss: 1.7069870233535767, validation_accuracy: 0.26980000734329224\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss: 1.839704155921936, validation_accuracy: 0.2800000011920929\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss: 1.7114845514297485, validation_accuracy: 0.28839999437332153\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 1.927111268043518, validation_accuracy: 0.2766000032424927\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss: 1.8127155303955078, validation_accuracy: 0.27959999442100525\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss: 1.6985887289047241, validation_accuracy: 0.2614000141620636\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss: 1.8747551441192627, validation_accuracy: 0.2694000005722046\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss: 1.7469253540039062, validation_accuracy: 0.2808000147342682\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 1.8802989721298218, validation_accuracy: 0.28679999709129333\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss: 1.7988239526748657, validation_accuracy: 0.25699999928474426\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss: 1.6027679443359375, validation_accuracy: 0.2856000065803528\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss: 1.7977091073989868, validation_accuracy: 0.29319998621940613\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss: 1.7096954584121704, validation_accuracy: 0.29339998960494995\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 1.8868755102157593, validation_accuracy: 0.29440000653266907\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss: 1.7383053302764893, validation_accuracy: 0.2930000126361847\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss: 1.5717793703079224, validation_accuracy: 0.3043999969959259\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss: 1.7712246179580688, validation_accuracy: 0.29840001463890076\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss: 1.7555763721466064, validation_accuracy: 0.22920000553131104\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 1.8258039951324463, validation_accuracy: 0.3068000078201294\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss: 1.7510263919830322, validation_accuracy: 0.27160000801086426\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss: 1.7025210857391357, validation_accuracy: 0.3027999997138977\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss: 1.7421125173568726, validation_accuracy: 0.3073999881744385\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss: 1.760690450668335, validation_accuracy: 0.28519999980926514\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 1.7961069345474243, validation_accuracy: 0.3147999942302704\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss: 1.7289444208145142, validation_accuracy: 0.27639999985694885\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss: 1.6023048162460327, validation_accuracy: 0.298799991607666\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss: 1.7889693975448608, validation_accuracy: 0.3009999990463257\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss: 1.7039175033569336, validation_accuracy: 0.27399998903274536\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 1.8075679540634155, validation_accuracy: 0.30079999566078186\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss: 1.7330013513565063, validation_accuracy: 0.32339999079704285\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss: 1.5796871185302734, validation_accuracy: 0.29600000381469727\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss: 1.7355684041976929, validation_accuracy: 0.3156000077724457\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss: 1.6873950958251953, validation_accuracy: 0.30059999227523804\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 1.769494652748108, validation_accuracy: 0.32359999418258667\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss: 1.689573049545288, validation_accuracy: 0.3203999996185303\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss: 1.560510516166687, validation_accuracy: 0.3043999969959259\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss: 1.7531251907348633, validation_accuracy: 0.2985999882221222\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss: 1.6577980518341064, validation_accuracy: 0.3269999921321869\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 1.7221591472625732, validation_accuracy: 0.32820001244544983\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss: 1.6801847219467163, validation_accuracy: 0.3264000117778778\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss: 1.5691726207733154, validation_accuracy: 0.31279999017715454\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss: 1.6958611011505127, validation_accuracy: 0.3400000035762787\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss: 1.6260368824005127, validation_accuracy: 0.32659998536109924\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 1.7614294290542603, validation_accuracy: 0.33160001039505005\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss: 1.6546701192855835, validation_accuracy: 0.3416000008583069\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss: 1.6083049774169922, validation_accuracy: 0.3165999948978424\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss: 1.7023117542266846, validation_accuracy: 0.3203999996185303\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss: 1.7177002429962158, validation_accuracy: 0.2906000018119812\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 1.7767353057861328, validation_accuracy: 0.3240000009536743\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss: 1.7230875492095947, validation_accuracy: 0.3208000063896179\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss: 1.612287163734436, validation_accuracy: 0.34279999136924744\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss: 1.7343858480453491, validation_accuracy: 0.31619998812675476\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss: 1.6443321704864502, validation_accuracy: 0.31119999289512634\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 1.7792692184448242, validation_accuracy: 0.32179999351501465\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss: 1.6552112102508545, validation_accuracy: 0.3206000030040741\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss: 1.6169674396514893, validation_accuracy: 0.32600000500679016\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss: 1.7332645654678345, validation_accuracy: 0.33719998598098755\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss: 1.6522047519683838, validation_accuracy: 0.3190000057220459\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 1.7015384435653687, validation_accuracy: 0.35679998993873596\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss: 1.6734707355499268, validation_accuracy: 0.33640000224113464\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss: 1.5403411388397217, validation_accuracy: 0.32820001244544983\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss: 1.6836551427841187, validation_accuracy: 0.34299999475479126\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss: 1.5837092399597168, validation_accuracy: 0.3440000116825104\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 1.7283923625946045, validation_accuracy: 0.3474000096321106\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss: 1.6355053186416626, validation_accuracy: 0.32580000162124634\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss: 1.553091287612915, validation_accuracy: 0.3434000015258789\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss: 1.6943655014038086, validation_accuracy: 0.3537999987602234\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss: 1.6386350393295288, validation_accuracy: 0.35519999265670776\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 1.6983673572540283, validation_accuracy: 0.3582000136375427\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss: 1.6535030603408813, validation_accuracy: 0.34060001373291016\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss: 1.5332297086715698, validation_accuracy: 0.3172000050544739\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss: 1.7355117797851562, validation_accuracy: 0.32260000705718994\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss: 1.6079342365264893, validation_accuracy: 0.3452000021934509\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 1.6567312479019165, validation_accuracy: 0.36340001225471497\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss: 1.5690463781356812, validation_accuracy: 0.3353999853134155\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss: 1.5273797512054443, validation_accuracy: 0.36399999260902405\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss: 1.7098515033721924, validation_accuracy: 0.3614000082015991\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss: 1.5798122882843018, validation_accuracy: 0.3547999858856201\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 1.6581395864486694, validation_accuracy: 0.3659999966621399\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss: 1.6230337619781494, validation_accuracy: 0.3531999886035919\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss: 1.5045047998428345, validation_accuracy: 0.3630000054836273\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss: 1.7228972911834717, validation_accuracy: 0.33719998598098755\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss: 1.6344133615493774, validation_accuracy: 0.35920000076293945\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 1.649539589881897, validation_accuracy: 0.3515999913215637\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss: 1.5873675346374512, validation_accuracy: 0.33219999074935913\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss: 1.4695930480957031, validation_accuracy: 0.3248000144958496\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss: 1.6668163537979126, validation_accuracy: 0.3246000111103058\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss: 1.57547128200531, validation_accuracy: 0.3646000027656555\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 1.6254539489746094, validation_accuracy: 0.37279999256134033\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss: 1.5791778564453125, validation_accuracy: 0.3617999851703644\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss: 1.500035047531128, validation_accuracy: 0.3601999878883362\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss: 1.6557750701904297, validation_accuracy: 0.3686000108718872\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss: 1.5376834869384766, validation_accuracy: 0.38659998774528503\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 1.626948595046997, validation_accuracy: 0.3783999979496002\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss: 1.5415407419204712, validation_accuracy: 0.3483999967575073\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss: 1.4942296743392944, validation_accuracy: 0.3840000033378601\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss: 1.6209373474121094, validation_accuracy: 0.3709999918937683\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss: 1.505980134010315, validation_accuracy: 0.3774000108242035\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 1.6336736679077148, validation_accuracy: 0.35499998927116394\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss: 1.5470561981201172, validation_accuracy: 0.3691999912261963\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss: 1.442155361175537, validation_accuracy: 0.3709999918937683\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss: 1.67314875125885, validation_accuracy: 0.365200012922287\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss: 1.4832861423492432, validation_accuracy: 0.39340001344680786\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 2.5038962364196777, validation_accuracy: 0.24400000274181366\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss: 1.5801845788955688, validation_accuracy: 0.3808000087738037\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss: 1.4391661882400513, validation_accuracy: 0.38679999113082886\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss: 1.6509926319122314, validation_accuracy: 0.3617999851703644\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss: 1.484951376914978, validation_accuracy: 0.3856000006198883\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 1.5757519006729126, validation_accuracy: 0.391400009393692\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss: 1.5630254745483398, validation_accuracy: 0.3625999987125397\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss: 1.4426342248916626, validation_accuracy: 0.37119999527931213\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss: 1.6340373754501343, validation_accuracy: 0.3643999993801117\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss: 1.4416427612304688, validation_accuracy: 0.3725999891757965\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 1.6076778173446655, validation_accuracy: 0.3513999879360199\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss: 1.5520713329315186, validation_accuracy: 0.3643999993801117\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss: 1.4332081079483032, validation_accuracy: 0.33219999074935913\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss: 1.590027093887329, validation_accuracy: 0.39500001072883606\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss: 1.425471544265747, validation_accuracy: 0.39480000734329224\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 1.5572489500045776, validation_accuracy: 0.39579999446868896\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss: 1.5441875457763672, validation_accuracy: 0.37279999256134033\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss: 1.4443011283874512, validation_accuracy: 0.39980000257492065\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss: 1.606629729270935, validation_accuracy: 0.37880000472068787\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss: 1.4410641193389893, validation_accuracy: 0.4065999984741211\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 1.5344879627227783, validation_accuracy: 0.40380001068115234\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss: 1.5281591415405273, validation_accuracy: 0.3758000135421753\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss: 1.4391049146652222, validation_accuracy: 0.36399999260902405\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss: 1.5700323581695557, validation_accuracy: 0.4007999897003174\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss: 1.4644076824188232, validation_accuracy: 0.3919999897480011\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 1.5298397541046143, validation_accuracy: 0.3901999890804291\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss: 1.543675184249878, validation_accuracy: 0.3747999966144562\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss: 1.4432365894317627, validation_accuracy: 0.3930000066757202\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss: 1.5568283796310425, validation_accuracy: 0.4023999869823456\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss: 1.4447335004806519, validation_accuracy: 0.39980000257492065\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 1.5178852081298828, validation_accuracy: 0.3977999985218048\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss: 1.512847661972046, validation_accuracy: 0.36500000953674316\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss: 1.4179813861846924, validation_accuracy: 0.3919999897480011\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss: 1.579019546508789, validation_accuracy: 0.40799999237060547\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss: 1.4207426309585571, validation_accuracy: 0.4034000039100647\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 1.5146502256393433, validation_accuracy: 0.4106000065803528\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss: 1.5022411346435547, validation_accuracy: 0.35899999737739563\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss: 1.4555184841156006, validation_accuracy: 0.3944000005722046\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss: 1.5511995553970337, validation_accuracy: 0.41359999775886536\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss: 1.4034671783447266, validation_accuracy: 0.39660000801086426\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 1.476529836654663, validation_accuracy: 0.40700000524520874\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss: 1.5220885276794434, validation_accuracy: 0.3490000069141388\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss: 1.449314832687378, validation_accuracy: 0.4000000059604645\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss: 1.5526793003082275, validation_accuracy: 0.397599995136261\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss: 1.4352788925170898, validation_accuracy: 0.39660000801086426\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 1.4922053813934326, validation_accuracy: 0.41760000586509705\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss: 1.4601387977600098, validation_accuracy: 0.3747999966144562\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss: 1.4240610599517822, validation_accuracy: 0.41179999709129333\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss: 1.55495285987854, validation_accuracy: 0.40720000863075256\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss: 1.4049333333969116, validation_accuracy: 0.4059999883174896\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 1.4477379322052002, validation_accuracy: 0.41179999709129333\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss: 1.496544599533081, validation_accuracy: 0.36000001430511475\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss: 1.425997257232666, validation_accuracy: 0.39259999990463257\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss: 1.5595753192901611, validation_accuracy: 0.3930000066757202\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss: 1.420844316482544, validation_accuracy: 0.39640000462532043\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 1.452686071395874, validation_accuracy: 0.41260001063346863\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss: 1.439828634262085, validation_accuracy: 0.3709999918937683\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss: 1.4402046203613281, validation_accuracy: 0.39079999923706055\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss: 1.5518951416015625, validation_accuracy: 0.40299999713897705\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss: 1.3906753063201904, validation_accuracy: 0.38920000195503235\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 1.4423855543136597, validation_accuracy: 0.41339999437332153\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss: 1.4226996898651123, validation_accuracy: 0.39480000734329224\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss: 1.4035152196884155, validation_accuracy: 0.397599995136261\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss: 1.541442632675171, validation_accuracy: 0.4020000100135803\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss: 1.389310359954834, validation_accuracy: 0.3937999904155731\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 1.4760984182357788, validation_accuracy: 0.41359999775886536\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss: 1.429829716682434, validation_accuracy: 0.3901999890804291\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss: 1.3728796243667603, validation_accuracy: 0.4065999984741211\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss: 1.5489141941070557, validation_accuracy: 0.4097999930381775\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss: 1.3847332000732422, validation_accuracy: 0.4027999937534332\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 1.4372737407684326, validation_accuracy: 0.41819998621940613\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss: 1.442740797996521, validation_accuracy: 0.37959998846054077\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss: 1.4146863222122192, validation_accuracy: 0.4004000127315521\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss: 1.5478065013885498, validation_accuracy: 0.40459999442100525\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss: 1.3627049922943115, validation_accuracy: 0.41100001335144043\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 1.4469661712646484, validation_accuracy: 0.41600000858306885\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss: 1.3718030452728271, validation_accuracy: 0.3871999979019165\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss: 1.3763554096221924, validation_accuracy: 0.41119998693466187\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss: 1.499858021736145, validation_accuracy: 0.41600000858306885\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss: 1.313623070716858, validation_accuracy: 0.415800005197525\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 1.439370036125183, validation_accuracy: 0.41019999980926514\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss: 1.3877179622650146, validation_accuracy: 0.3880000114440918\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss: 1.3359323740005493, validation_accuracy: 0.4244000017642975\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss: 1.4856281280517578, validation_accuracy: 0.4212000072002411\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss: 1.3137201070785522, validation_accuracy: 0.42340001463890076\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 1.4257901906967163, validation_accuracy: 0.4253999888896942\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss: 1.348496675491333, validation_accuracy: 0.4099999964237213\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss: 1.3370414972305298, validation_accuracy: 0.428600013256073\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss: 1.4507148265838623, validation_accuracy: 0.4357999861240387\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss: 1.2776583433151245, validation_accuracy: 0.42480000853538513\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 1.4082844257354736, validation_accuracy: 0.43560001254081726\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss: 1.3921895027160645, validation_accuracy: 0.415800005197525\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss: 1.3258025646209717, validation_accuracy: 0.44040000438690186\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss: 1.4428895711898804, validation_accuracy: 0.43560001254081726\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss: 1.2589083909988403, validation_accuracy: 0.4381999969482422\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 1.3614397048950195, validation_accuracy: 0.43540000915527344\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss: 1.3317722082138062, validation_accuracy: 0.42500001192092896\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss: 1.3288028240203857, validation_accuracy: 0.44179999828338623\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss: 1.4149378538131714, validation_accuracy: 0.43939998745918274\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss: 1.2178614139556885, validation_accuracy: 0.4519999921321869\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 1.2707679271697998, validation_accuracy: 0.4307999908924103\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss: 1.3253052234649658, validation_accuracy: 0.4429999887943268\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss: 1.2554433345794678, validation_accuracy: 0.4449999928474426\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss: 1.351258635520935, validation_accuracy: 0.45239999890327454\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss: 1.2416481971740723, validation_accuracy: 0.44760000705718994\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 1.2918980121612549, validation_accuracy: 0.4406000077724457\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss: 1.2702975273132324, validation_accuracy: 0.4544000029563904\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss: 1.2920061349868774, validation_accuracy: 0.4519999921321869\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss: 1.3190834522247314, validation_accuracy: 0.44940000772476196\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss: 1.1812152862548828, validation_accuracy: 0.45820000767707825\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 1.2435991764068604, validation_accuracy: 0.4392000138759613\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss: 1.2583773136138916, validation_accuracy: 0.45719999074935913\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss: 1.2630561590194702, validation_accuracy: 0.460999995470047\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss: 1.290126919746399, validation_accuracy: 0.4717999994754791\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss: 1.194481611251831, validation_accuracy: 0.4596000015735626\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 1.2347065210342407, validation_accuracy: 0.4586000144481659\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss: 1.2175482511520386, validation_accuracy: 0.4749999940395355\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss: 1.2464609146118164, validation_accuracy: 0.4666000008583069\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss: 1.2991992235183716, validation_accuracy: 0.46959999203681946\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss: 1.1322832107543945, validation_accuracy: 0.47040000557899475\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 1.2092978954315186, validation_accuracy: 0.4408000111579895\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss: 1.2330111265182495, validation_accuracy: 0.4652000069618225\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss: 1.210396409034729, validation_accuracy: 0.4657999873161316\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss: 1.3456891775131226, validation_accuracy: 0.46700000762939453\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss: 1.1073112487792969, validation_accuracy: 0.47920000553131104\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 1.1641778945922852, validation_accuracy: 0.46860000491142273\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss: 1.1851763725280762, validation_accuracy: 0.46619999408721924\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss: 1.232802391052246, validation_accuracy: 0.4521999955177307\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss: 1.2830222845077515, validation_accuracy: 0.4797999858856201\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss: 1.1543794870376587, validation_accuracy: 0.4729999899864197\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 1.1781708002090454, validation_accuracy: 0.4666000008583069\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss: 1.1263585090637207, validation_accuracy: 0.462799996137619\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss: 1.192620873451233, validation_accuracy: 0.47360000014305115\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss: 1.2658237218856812, validation_accuracy: 0.4722000062465668\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss: 1.1122411489486694, validation_accuracy: 0.48260000348091125\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 1.1614243984222412, validation_accuracy: 0.47780001163482666\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss: 1.1482059955596924, validation_accuracy: 0.47040000557899475\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss: 1.181471347808838, validation_accuracy: 0.4505999982357025\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss: 1.2614214420318604, validation_accuracy: 0.47360000014305115\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss: 1.1603507995605469, validation_accuracy: 0.45260000228881836\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 1.1889050006866455, validation_accuracy: 0.4562000036239624\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss: 1.1291773319244385, validation_accuracy: 0.4575999975204468\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss: 1.1864410638809204, validation_accuracy: 0.4620000123977661\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss: 1.2273720502853394, validation_accuracy: 0.4731999933719635\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss: 1.1260793209075928, validation_accuracy: 0.4846000075340271\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 1.1482372283935547, validation_accuracy: 0.4569999873638153\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss: 1.1300837993621826, validation_accuracy: 0.44119998812675476\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss: 1.1282811164855957, validation_accuracy: 0.47839999198913574\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss: 1.232922911643982, validation_accuracy: 0.48019999265670776\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss: 1.095453143119812, validation_accuracy: 0.47600001096725464\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 1.1259796619415283, validation_accuracy: 0.46619999408721924\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss: 1.0989997386932373, validation_accuracy: 0.4745999872684479\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss: 1.1713662147521973, validation_accuracy: 0.4625999927520752\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss: 1.2238725423812866, validation_accuracy: 0.4733999967575073\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss: 1.1023082733154297, validation_accuracy: 0.47360000014305115\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 1.128570556640625, validation_accuracy: 0.4803999960422516\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss: 1.0876636505126953, validation_accuracy: 0.4715999960899353\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss: 1.1411832571029663, validation_accuracy: 0.4726000130176544\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss: 1.291378378868103, validation_accuracy: 0.45399999618530273\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss: 1.1010777950286865, validation_accuracy: 0.48739999532699585\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 1.124064326286316, validation_accuracy: 0.4772000014781952\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss: 1.0801804065704346, validation_accuracy: 0.4602000117301941\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss: 1.1502045392990112, validation_accuracy: 0.4652000069618225\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss: 1.2230539321899414, validation_accuracy: 0.48019999265670776\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss: 1.0834087133407593, validation_accuracy: 0.48420000076293945\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 1.1467552185058594, validation_accuracy: 0.47099998593330383\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss: 1.071699857711792, validation_accuracy: 0.487199991941452\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss: 1.1260367631912231, validation_accuracy: 0.4717999994754791\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss: 1.204974889755249, validation_accuracy: 0.4790000021457672\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss: 1.0699563026428223, validation_accuracy: 0.4796000123023987\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 1.069068193435669, validation_accuracy: 0.49140000343322754\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss: 1.046942949295044, validation_accuracy: 0.4966000020503998\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss: 1.106872797012329, validation_accuracy: 0.46459999680519104\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss: 1.2157402038574219, validation_accuracy: 0.462799996137619\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss: 1.0586168766021729, validation_accuracy: 0.48899999260902405\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 1.0953174829483032, validation_accuracy: 0.492000013589859\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss: 1.0370718240737915, validation_accuracy: 0.4934000074863434\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss: 1.0951340198516846, validation_accuracy: 0.47620001435279846\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss: 1.2029274702072144, validation_accuracy: 0.4726000130176544\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss: 1.0255945920944214, validation_accuracy: 0.487199991941452\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 1.1113927364349365, validation_accuracy: 0.4781999886035919\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss: 1.0377376079559326, validation_accuracy: 0.4927999973297119\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss: 1.1133005619049072, validation_accuracy: 0.4702000021934509\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss: 1.176823377609253, validation_accuracy: 0.48579999804496765\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss: 1.0749484300613403, validation_accuracy: 0.4731999933719635\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 1.1040880680084229, validation_accuracy: 0.47839999198913574\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss: 1.0276920795440674, validation_accuracy: 0.48739999532699585\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss: 1.085286259651184, validation_accuracy: 0.48260000348091125\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss: 1.2733386754989624, validation_accuracy: 0.42739999294281006\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss: 1.0526936054229736, validation_accuracy: 0.47780001163482666\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 1.079345464706421, validation_accuracy: 0.48840001225471497\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss: 1.0241658687591553, validation_accuracy: 0.49959999322891235\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss: 1.0654346942901611, validation_accuracy: 0.46939998865127563\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss: 1.1651663780212402, validation_accuracy: 0.4708000123500824\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss: 1.0171544551849365, validation_accuracy: 0.4848000109195709\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 1.078186273574829, validation_accuracy: 0.49000000953674316\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss: 1.0507197380065918, validation_accuracy: 0.49959999322891235\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss: 1.0721486806869507, validation_accuracy: 0.4765999913215637\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss: 1.1602303981781006, validation_accuracy: 0.47040000557899475\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss: 0.9855982065200806, validation_accuracy: 0.49480000138282776\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 1.0721616744995117, validation_accuracy: 0.4844000041484833\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss: 1.0370863676071167, validation_accuracy: 0.5012000203132629\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss: 1.0494935512542725, validation_accuracy: 0.48080000281333923\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss: 1.161928415298462, validation_accuracy: 0.4772000014781952\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss: 1.0259023904800415, validation_accuracy: 0.48260000348091125\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 1.063382863998413, validation_accuracy: 0.4749999940395355\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss: 1.0025036334991455, validation_accuracy: 0.504800021648407\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss: 1.0388007164001465, validation_accuracy: 0.4657999873161316\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss: 1.1725809574127197, validation_accuracy: 0.4779999852180481\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss: 0.9870249629020691, validation_accuracy: 0.49300000071525574\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 1.0718357563018799, validation_accuracy: 0.48739999532699585\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss: 0.9855683445930481, validation_accuracy: 0.5019999742507935\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss: 1.030745029449463, validation_accuracy: 0.48159998655319214\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss: 1.1456763744354248, validation_accuracy: 0.49000000953674316\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss: 0.9663766622543335, validation_accuracy: 0.4975999891757965\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 1.0639032125473022, validation_accuracy: 0.47440001368522644\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss: 1.0680811405181885, validation_accuracy: 0.48240000009536743\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss: 1.0467859506607056, validation_accuracy: 0.4742000102996826\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss: 1.170495867729187, validation_accuracy: 0.4726000130176544\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss: 0.9501340985298157, validation_accuracy: 0.5004000067710876\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 1.0487111806869507, validation_accuracy: 0.49239999055862427\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss: 0.9946576952934265, validation_accuracy: 0.5049999952316284\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss: 1.051539421081543, validation_accuracy: 0.4740000069141388\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss: 1.200081467628479, validation_accuracy: 0.44679999351501465\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss: 0.9684742093086243, validation_accuracy: 0.49939998984336853\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 1.0434688329696655, validation_accuracy: 0.49079999327659607\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss: 1.1230653524398804, validation_accuracy: 0.47519999742507935\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss: 1.040509819984436, validation_accuracy: 0.48260000348091125\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss: 1.100551724433899, validation_accuracy: 0.4936000108718872\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss: 0.9199317693710327, validation_accuracy: 0.4991999864578247\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 1.0383917093276978, validation_accuracy: 0.49799999594688416\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss: 1.0297234058380127, validation_accuracy: 0.5072000026702881\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss: 0.9904616475105286, validation_accuracy: 0.4844000041484833\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss: 1.1095043420791626, validation_accuracy: 0.4878000020980835\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss: 0.908785343170166, validation_accuracy: 0.5026000142097473\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 1.0344537496566772, validation_accuracy: 0.4968000054359436\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss: 1.0148470401763916, validation_accuracy: 0.4925999939441681\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss: 0.9644078016281128, validation_accuracy: 0.503600001335144\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss: 1.12607741355896, validation_accuracy: 0.498199999332428\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss: 0.9122732877731323, validation_accuracy: 0.5062000155448914\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 1.015393853187561, validation_accuracy: 0.5099999904632568\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss: 0.981890857219696, validation_accuracy: 0.5088000297546387\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss: 1.0028775930404663, validation_accuracy: 0.4909999966621399\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss: 1.0894622802734375, validation_accuracy: 0.4803999960422516\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss: 0.8785120248794556, validation_accuracy: 0.517799973487854\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 1.0022891759872437, validation_accuracy: 0.5112000107765198\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss: 0.9694535136222839, validation_accuracy: 0.5148000121116638\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss: 0.9533987045288086, validation_accuracy: 0.5121999979019165\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss: 1.0865497589111328, validation_accuracy: 0.498199999332428\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss: 0.8839422464370728, validation_accuracy: 0.5228000283241272\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 0.9813498258590698, validation_accuracy: 0.5113999843597412\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss: 0.9337363243103027, validation_accuracy: 0.5138000249862671\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss: 0.9157642126083374, validation_accuracy: 0.49459999799728394\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss: 1.082031488418579, validation_accuracy: 0.5022000074386597\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss: 0.9044899940490723, validation_accuracy: 0.5095999836921692\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 0.9596721529960632, validation_accuracy: 0.5180000066757202\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss: 0.945547878742218, validation_accuracy: 0.5152000188827515\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss: 0.9610399007797241, validation_accuracy: 0.5094000101089478\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss: 1.028709053993225, validation_accuracy: 0.5099999904632568\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss: 0.8552826046943665, validation_accuracy: 0.5153999924659729\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 0.9670049548149109, validation_accuracy: 0.5139999985694885\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss: 0.9012230038642883, validation_accuracy: 0.519599974155426\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss: 0.9242056608200073, validation_accuracy: 0.49799999594688416\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss: 1.049907922744751, validation_accuracy: 0.5109999775886536\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss: 0.8476656079292297, validation_accuracy: 0.5289999842643738\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 0.9642825126647949, validation_accuracy: 0.5350000262260437\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss: 0.8917974233627319, validation_accuracy: 0.5242000222206116\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss: 0.869590163230896, validation_accuracy: 0.5027999877929688\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss: 1.0294361114501953, validation_accuracy: 0.5080000162124634\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss: 0.8461977243423462, validation_accuracy: 0.5270000100135803\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 0.9622384309768677, validation_accuracy: 0.5238000154495239\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss: 0.8915833234786987, validation_accuracy: 0.5239999890327454\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss: 0.8708349466323853, validation_accuracy: 0.503600001335144\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss: 1.018998622894287, validation_accuracy: 0.5156000256538391\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss: 0.8134441375732422, validation_accuracy: 0.5320000052452087\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 0.9293969869613647, validation_accuracy: 0.5260000228881836\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss: 0.8560962677001953, validation_accuracy: 0.521399974822998\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss: 0.8772543668746948, validation_accuracy: 0.49619999527931213\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss: 1.0097002983093262, validation_accuracy: 0.5221999883651733\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss: 0.8150327801704407, validation_accuracy: 0.5275999903678894\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 0.9155477285385132, validation_accuracy: 0.5170000195503235\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss: 0.9063995480537415, validation_accuracy: 0.5139999985694885\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss: 0.8750173449516296, validation_accuracy: 0.49939998984336853\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss: 1.0209367275238037, validation_accuracy: 0.5230000019073486\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss: 0.7904926538467407, validation_accuracy: 0.5148000121116638\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 0.9228619337081909, validation_accuracy: 0.5270000100135803\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss: 0.8478549122810364, validation_accuracy: 0.524399995803833\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss: 0.8918222188949585, validation_accuracy: 0.5080000162124634\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss: 1.0151493549346924, validation_accuracy: 0.5271999835968018\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss: 0.7848773002624512, validation_accuracy: 0.5296000242233276\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 0.9029237627983093, validation_accuracy: 0.52920001745224\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss: 0.8075944185256958, validation_accuracy: 0.5278000235557556\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss: 0.8806278109550476, validation_accuracy: 0.508400022983551\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss: 0.9980270266532898, validation_accuracy: 0.5153999924659729\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss: 0.774206817150116, validation_accuracy: 0.5383999943733215\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 0.907914936542511, validation_accuracy: 0.5228000283241272\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss: 0.8255144357681274, validation_accuracy: 0.5353999733924866\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss: 0.8476263284683228, validation_accuracy: 0.5267999768257141\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss: 0.9766858816146851, validation_accuracy: 0.5246000289916992\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss: 0.7623311281204224, validation_accuracy: 0.5368000268936157\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 0.8483865857124329, validation_accuracy: 0.5289999842643738\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss: 0.857723593711853, validation_accuracy: 0.5217999815940857\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss: 0.7739547491073608, validation_accuracy: 0.5320000052452087\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss: 0.9499666094779968, validation_accuracy: 0.527999997138977\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss: 0.7160763144493103, validation_accuracy: 0.5332000255584717\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 0.8395937085151672, validation_accuracy: 0.5365999937057495\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss: 0.7767308354377747, validation_accuracy: 0.534600019454956\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss: 0.806313157081604, validation_accuracy: 0.526199996471405\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss: 0.9326942563056946, validation_accuracy: 0.5234000086784363\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss: 0.7410303950309753, validation_accuracy: 0.5401999950408936\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 0.8607051968574524, validation_accuracy: 0.5424000024795532\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss: 0.7815644145011902, validation_accuracy: 0.5342000126838684\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss: 0.8103909492492676, validation_accuracy: 0.5306000113487244\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss: 0.9158594012260437, validation_accuracy: 0.5297999978065491\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss: 0.6787070035934448, validation_accuracy: 0.5418000221252441\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 0.8409818410873413, validation_accuracy: 0.5383999943733215\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss: 0.7435340881347656, validation_accuracy: 0.5382000207901001\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss: 0.7902176380157471, validation_accuracy: 0.5371999740600586\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss: 0.906734824180603, validation_accuracy: 0.5270000100135803\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss: 0.6899963617324829, validation_accuracy: 0.5410000085830688\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 0.8219865560531616, validation_accuracy: 0.5393999814987183\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss: 0.7382453680038452, validation_accuracy: 0.5324000120162964\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss: 0.7560974955558777, validation_accuracy: 0.5351999998092651\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss: 0.8814611434936523, validation_accuracy: 0.5252000093460083\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss: 0.6649261713027954, validation_accuracy: 0.5465999841690063\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 0.8133420944213867, validation_accuracy: 0.5343999862670898\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss: 0.721957802772522, validation_accuracy: 0.5368000268936157\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss: 0.7485339045524597, validation_accuracy: 0.5533999800682068\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss: 0.8777763247489929, validation_accuracy: 0.525600016117096\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss: 0.6522994041442871, validation_accuracy: 0.5429999828338623\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 0.7959001064300537, validation_accuracy: 0.5406000018119812\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss: 0.7357253432273865, validation_accuracy: 0.5392000079154968\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss: 0.7415138483047485, validation_accuracy: 0.5375999808311462\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss: 0.8747056722640991, validation_accuracy: 0.5347999930381775\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss: 0.6730345487594604, validation_accuracy: 0.5392000079154968\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 0.787376880645752, validation_accuracy: 0.5351999998092651\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss: 0.6976369619369507, validation_accuracy: 0.5388000011444092\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss: 0.7478697896003723, validation_accuracy: 0.5389999747276306\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss: 0.8673185110092163, validation_accuracy: 0.5235999822616577\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss: 0.6498846411705017, validation_accuracy: 0.5496000051498413\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 0.7735695242881775, validation_accuracy: 0.5388000011444092\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss: 0.703215479850769, validation_accuracy: 0.5410000085830688\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss: 0.7255385518074036, validation_accuracy: 0.5475999712944031\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss: 0.8422839045524597, validation_accuracy: 0.5270000100135803\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss: 0.6475393176078796, validation_accuracy: 0.5465999841690063\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 0.792236328125, validation_accuracy: 0.5490000247955322\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss: 0.6916581392288208, validation_accuracy: 0.5414000153541565\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss: 0.7752028703689575, validation_accuracy: 0.5230000019073486\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss: 0.8543203473091125, validation_accuracy: 0.5302000045776367\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss: 0.6535954475402832, validation_accuracy: 0.5440000295639038\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 0.7526686787605286, validation_accuracy: 0.5357999801635742\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss: 0.6807687282562256, validation_accuracy: 0.5375999808311462\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss: 0.7312902212142944, validation_accuracy: 0.5464000105857849\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss: 0.8206031918525696, validation_accuracy: 0.5167999863624573\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss: 0.6461876630783081, validation_accuracy: 0.5508000254631042\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 0.7589507102966309, validation_accuracy: 0.5522000193595886\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss: 0.6706196069717407, validation_accuracy: 0.5393999814987183\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss: 0.7172027230262756, validation_accuracy: 0.5388000011444092\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss: 0.8461408615112305, validation_accuracy: 0.5157999992370605\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss: 0.6911227107048035, validation_accuracy: 0.5486000180244446\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 0.7722679376602173, validation_accuracy: 0.5429999828338623\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss: 0.6848397254943848, validation_accuracy: 0.5379999876022339\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss: 0.7217567563056946, validation_accuracy: 0.5357999801635742\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss: 0.8288793563842773, validation_accuracy: 0.5401999950408936\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss: 0.602004885673523, validation_accuracy: 0.5514000058174133\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 0.7447820901870728, validation_accuracy: 0.5508000254631042\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss: 0.672687828540802, validation_accuracy: 0.534600019454956\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss: 0.7077386379241943, validation_accuracy: 0.5315999984741211\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss: 0.7922974824905396, validation_accuracy: 0.5224000215530396\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss: 0.6125293374061584, validation_accuracy: 0.5504000186920166\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 0.7278228402137756, validation_accuracy: 0.5379999876022339\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss: 0.6949875354766846, validation_accuracy: 0.5171999931335449\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss: 0.6877515316009521, validation_accuracy: 0.5406000018119812\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss: 0.8130590319633484, validation_accuracy: 0.5293999910354614\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss: 0.6404222846031189, validation_accuracy: 0.5473999977111816\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 0.7238286137580872, validation_accuracy: 0.5428000092506409\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss: 0.6222854852676392, validation_accuracy: 0.5389999747276306\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss: 0.691630482673645, validation_accuracy: 0.5378000140190125\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss: 0.7765101790428162, validation_accuracy: 0.5364000201225281\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss: 0.5824621319770813, validation_accuracy: 0.545799970626831\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss: 0.6834998726844788, validation_accuracy: 0.5360000133514404\n",
      "Epoch 101, CIFAR-10 Batch 2:  loss: 0.628577709197998, validation_accuracy: 0.54339998960495\n",
      "Epoch 101, CIFAR-10 Batch 3:  loss: 0.6885351538658142, validation_accuracy: 0.5407999753952026\n",
      "Epoch 101, CIFAR-10 Batch 4:  loss: 0.7764636278152466, validation_accuracy: 0.5432000160217285\n",
      "Epoch 101, CIFAR-10 Batch 5:  loss: 0.5369092226028442, validation_accuracy: 0.5422000288963318\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss: 0.6975099444389343, validation_accuracy: 0.5468000173568726\n",
      "Epoch 102, CIFAR-10 Batch 2:  loss: 0.5864772796630859, validation_accuracy: 0.5455999970436096\n",
      "Epoch 102, CIFAR-10 Batch 3:  loss: 0.6487182974815369, validation_accuracy: 0.553600013256073\n",
      "Epoch 102, CIFAR-10 Batch 4:  loss: 0.7787141799926758, validation_accuracy: 0.5382000207901001\n",
      "Epoch 102, CIFAR-10 Batch 5:  loss: 0.5370675325393677, validation_accuracy: 0.5460000038146973\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss: 0.6910004615783691, validation_accuracy: 0.5540000200271606\n",
      "Epoch 103, CIFAR-10 Batch 2:  loss: 0.5838657021522522, validation_accuracy: 0.5465999841690063\n",
      "Epoch 103, CIFAR-10 Batch 3:  loss: 0.6584901809692383, validation_accuracy: 0.5501999855041504\n",
      "Epoch 103, CIFAR-10 Batch 4:  loss: 0.7647495269775391, validation_accuracy: 0.5311999917030334\n",
      "Epoch 103, CIFAR-10 Batch 5:  loss: 0.5267917513847351, validation_accuracy: 0.5446000099182129\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss: 0.6888953447341919, validation_accuracy: 0.5424000024795532\n",
      "Epoch 104, CIFAR-10 Batch 2:  loss: 0.5381404161453247, validation_accuracy: 0.5473999977111816\n",
      "Epoch 104, CIFAR-10 Batch 3:  loss: 0.666793942451477, validation_accuracy: 0.5468000173568726\n",
      "Epoch 104, CIFAR-10 Batch 4:  loss: 0.7564516067504883, validation_accuracy: 0.5388000011444092\n",
      "Epoch 104, CIFAR-10 Batch 5:  loss: 0.49565544724464417, validation_accuracy: 0.5404000282287598\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss: 0.7048858404159546, validation_accuracy: 0.5465999841690063\n",
      "Epoch 105, CIFAR-10 Batch 2:  loss: 0.5535656809806824, validation_accuracy: 0.545199990272522\n",
      "Epoch 105, CIFAR-10 Batch 3:  loss: 0.6120058298110962, validation_accuracy: 0.5424000024795532\n",
      "Epoch 105, CIFAR-10 Batch 4:  loss: 0.751972496509552, validation_accuracy: 0.5400000214576721\n",
      "Epoch 105, CIFAR-10 Batch 5:  loss: 0.500400185585022, validation_accuracy: 0.5442000031471252\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss: 0.6600855588912964, validation_accuracy: 0.5432000160217285\n",
      "Epoch 106, CIFAR-10 Batch 2:  loss: 0.5692389607429504, validation_accuracy: 0.5374000072479248\n",
      "Epoch 106, CIFAR-10 Batch 3:  loss: 0.6467561721801758, validation_accuracy: 0.5461999773979187\n",
      "Epoch 106, CIFAR-10 Batch 4:  loss: 0.7148280143737793, validation_accuracy: 0.5419999957084656\n",
      "Epoch 106, CIFAR-10 Batch 5:  loss: 0.5100222229957581, validation_accuracy: 0.5446000099182129\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss: 0.6381387710571289, validation_accuracy: 0.5564000010490417\n",
      "Epoch 107, CIFAR-10 Batch 2:  loss: 0.5513363480567932, validation_accuracy: 0.5446000099182129\n",
      "Epoch 107, CIFAR-10 Batch 3:  loss: 0.6162988543510437, validation_accuracy: 0.5541999936103821\n",
      "Epoch 107, CIFAR-10 Batch 4:  loss: 0.7229105234146118, validation_accuracy: 0.5442000031471252\n",
      "Epoch 107, CIFAR-10 Batch 5:  loss: 0.5066252946853638, validation_accuracy: 0.5479999780654907\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss: 0.6178693175315857, validation_accuracy: 0.5527999997138977\n",
      "Epoch 108, CIFAR-10 Batch 2:  loss: 0.5407053232192993, validation_accuracy: 0.5532000064849854\n",
      "Epoch 108, CIFAR-10 Batch 3:  loss: 0.6016632318496704, validation_accuracy: 0.5490000247955322\n",
      "Epoch 108, CIFAR-10 Batch 4:  loss: 0.690258264541626, validation_accuracy: 0.5440000295639038\n",
      "Epoch 108, CIFAR-10 Batch 5:  loss: 0.4604624807834625, validation_accuracy: 0.5450000166893005\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss: 0.658734917640686, validation_accuracy: 0.5419999957084656\n",
      "Epoch 109, CIFAR-10 Batch 2:  loss: 0.5699117183685303, validation_accuracy: 0.553600013256073\n",
      "Epoch 109, CIFAR-10 Batch 3:  loss: 0.6314029693603516, validation_accuracy: 0.5586000084877014\n",
      "Epoch 109, CIFAR-10 Batch 4:  loss: 0.6806300282478333, validation_accuracy: 0.545799970626831\n",
      "Epoch 109, CIFAR-10 Batch 5:  loss: 0.4906502664089203, validation_accuracy: 0.5537999868392944\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss: 0.605411171913147, validation_accuracy: 0.5613999962806702\n",
      "Epoch 110, CIFAR-10 Batch 2:  loss: 0.5145922303199768, validation_accuracy: 0.5591999888420105\n",
      "Epoch 110, CIFAR-10 Batch 3:  loss: 0.5848435163497925, validation_accuracy: 0.5641999840736389\n",
      "Epoch 110, CIFAR-10 Batch 4:  loss: 0.6630140542984009, validation_accuracy: 0.5540000200271606\n",
      "Epoch 110, CIFAR-10 Batch 5:  loss: 0.4639299511909485, validation_accuracy: 0.5527999997138977\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss: 0.5808539390563965, validation_accuracy: 0.5623999834060669\n",
      "Epoch 111, CIFAR-10 Batch 2:  loss: 0.48996108770370483, validation_accuracy: 0.5649999976158142\n",
      "Epoch 111, CIFAR-10 Batch 3:  loss: 0.5444070100784302, validation_accuracy: 0.5640000104904175\n",
      "Epoch 111, CIFAR-10 Batch 4:  loss: 0.6244961619377136, validation_accuracy: 0.5519999861717224\n",
      "Epoch 111, CIFAR-10 Batch 5:  loss: 0.5180553197860718, validation_accuracy: 0.555400013923645\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss: 0.6002122759819031, validation_accuracy: 0.5630000233650208\n",
      "Epoch 112, CIFAR-10 Batch 2:  loss: 0.4693218171596527, validation_accuracy: 0.5591999888420105\n",
      "Epoch 112, CIFAR-10 Batch 3:  loss: 0.5079078674316406, validation_accuracy: 0.5626000165939331\n",
      "Epoch 112, CIFAR-10 Batch 4:  loss: 0.6358789205551147, validation_accuracy: 0.5515999794006348\n",
      "Epoch 112, CIFAR-10 Batch 5:  loss: 0.469529926776886, validation_accuracy: 0.5537999868392944\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss: 0.5687006115913391, validation_accuracy: 0.5577999949455261\n",
      "Epoch 113, CIFAR-10 Batch 2:  loss: 0.4729262888431549, validation_accuracy: 0.5564000010490417\n",
      "Epoch 113, CIFAR-10 Batch 3:  loss: 0.5189485549926758, validation_accuracy: 0.5680000185966492\n",
      "Epoch 113, CIFAR-10 Batch 4:  loss: 0.5609328150749207, validation_accuracy: 0.5626000165939331\n",
      "Epoch 113, CIFAR-10 Batch 5:  loss: 0.4761575758457184, validation_accuracy: 0.5608000159263611\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss: 0.5924809575080872, validation_accuracy: 0.5640000104904175\n",
      "Epoch 114, CIFAR-10 Batch 2:  loss: 0.45868635177612305, validation_accuracy: 0.5648000240325928\n",
      "Epoch 114, CIFAR-10 Batch 3:  loss: 0.48444265127182007, validation_accuracy: 0.5640000104904175\n",
      "Epoch 114, CIFAR-10 Batch 4:  loss: 0.5640765428543091, validation_accuracy: 0.5526000261306763\n",
      "Epoch 114, CIFAR-10 Batch 5:  loss: 0.4554097056388855, validation_accuracy: 0.5393999814987183\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss: 0.5983361005783081, validation_accuracy: 0.5612000226974487\n",
      "Epoch 115, CIFAR-10 Batch 2:  loss: 0.4650130867958069, validation_accuracy: 0.5587999820709229\n",
      "Epoch 115, CIFAR-10 Batch 3:  loss: 0.4584997296333313, validation_accuracy: 0.5532000064849854\n",
      "Epoch 115, CIFAR-10 Batch 4:  loss: 0.5683767795562744, validation_accuracy: 0.5684000253677368\n",
      "Epoch 115, CIFAR-10 Batch 5:  loss: 0.4480739235877991, validation_accuracy: 0.5658000111579895\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss: 0.5237147808074951, validation_accuracy: 0.5748000144958496\n",
      "Epoch 116, CIFAR-10 Batch 2:  loss: 0.4403035044670105, validation_accuracy: 0.5586000084877014\n",
      "Epoch 116, CIFAR-10 Batch 3:  loss: 0.4918852746486664, validation_accuracy: 0.5649999976158142\n",
      "Epoch 116, CIFAR-10 Batch 4:  loss: 0.538110077381134, validation_accuracy: 0.5630000233650208\n",
      "Epoch 116, CIFAR-10 Batch 5:  loss: 0.4449787139892578, validation_accuracy: 0.5582000017166138\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss: 0.5126354098320007, validation_accuracy: 0.5784000158309937\n",
      "Epoch 117, CIFAR-10 Batch 2:  loss: 0.4344286024570465, validation_accuracy: 0.5703999996185303\n",
      "Epoch 117, CIFAR-10 Batch 3:  loss: 0.446258544921875, validation_accuracy: 0.5496000051498413\n",
      "Epoch 117, CIFAR-10 Batch 4:  loss: 0.5421698689460754, validation_accuracy: 0.5519999861717224\n",
      "Epoch 117, CIFAR-10 Batch 5:  loss: 0.4705486297607422, validation_accuracy: 0.5633999705314636\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss: 0.5512338876724243, validation_accuracy: 0.573199987411499\n",
      "Epoch 118, CIFAR-10 Batch 2:  loss: 0.42911046743392944, validation_accuracy: 0.5564000010490417\n",
      "Epoch 118, CIFAR-10 Batch 3:  loss: 0.4388789236545563, validation_accuracy: 0.5529999732971191\n",
      "Epoch 118, CIFAR-10 Batch 4:  loss: 0.48485565185546875, validation_accuracy: 0.5609999895095825\n",
      "Epoch 118, CIFAR-10 Batch 5:  loss: 0.479409784078598, validation_accuracy: 0.5623999834060669\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss: 0.5184215903282166, validation_accuracy: 0.5666000247001648\n",
      "Epoch 119, CIFAR-10 Batch 2:  loss: 0.43100324273109436, validation_accuracy: 0.5662000179290771\n",
      "Epoch 119, CIFAR-10 Batch 3:  loss: 0.4347459673881531, validation_accuracy: 0.5618000030517578\n",
      "Epoch 119, CIFAR-10 Batch 4:  loss: 0.4674747586250305, validation_accuracy: 0.5532000064849854\n",
      "Epoch 119, CIFAR-10 Batch 5:  loss: 0.43912068009376526, validation_accuracy: 0.5616000294685364\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss: 0.5345596671104431, validation_accuracy: 0.5630000233650208\n",
      "Epoch 120, CIFAR-10 Batch 2:  loss: 0.45486101508140564, validation_accuracy: 0.5577999949455261\n",
      "Epoch 120, CIFAR-10 Batch 3:  loss: 0.4529997706413269, validation_accuracy: 0.5464000105857849\n",
      "Epoch 120, CIFAR-10 Batch 4:  loss: 0.45998650789260864, validation_accuracy: 0.5654000043869019\n",
      "Epoch 120, CIFAR-10 Batch 5:  loss: 0.4664026200771332, validation_accuracy: 0.5626000165939331\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss: 0.5207298994064331, validation_accuracy: 0.5644000172615051\n",
      "Epoch 121, CIFAR-10 Batch 2:  loss: 0.43889111280441284, validation_accuracy: 0.5645999908447266\n",
      "Epoch 121, CIFAR-10 Batch 3:  loss: 0.4475153982639313, validation_accuracy: 0.5550000071525574\n",
      "Epoch 121, CIFAR-10 Batch 4:  loss: 0.4332355558872223, validation_accuracy: 0.557200014591217\n",
      "Epoch 121, CIFAR-10 Batch 5:  loss: 0.44213706254959106, validation_accuracy: 0.5666000247001648\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss: 0.4838670790195465, validation_accuracy: 0.5741999745368958\n",
      "Epoch 122, CIFAR-10 Batch 2:  loss: 0.3901587128639221, validation_accuracy: 0.5633999705314636\n",
      "Epoch 122, CIFAR-10 Batch 3:  loss: 0.43294304609298706, validation_accuracy: 0.551800012588501\n",
      "Epoch 122, CIFAR-10 Batch 4:  loss: 0.4306933283805847, validation_accuracy: 0.5595999956130981\n",
      "Epoch 122, CIFAR-10 Batch 5:  loss: 0.4640832543373108, validation_accuracy: 0.5555999875068665\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss: 0.5298444032669067, validation_accuracy: 0.5605999827384949\n",
      "Epoch 123, CIFAR-10 Batch 2:  loss: 0.43143048882484436, validation_accuracy: 0.567799985408783\n",
      "Epoch 123, CIFAR-10 Batch 3:  loss: 0.42531824111938477, validation_accuracy: 0.5651999711990356\n",
      "Epoch 123, CIFAR-10 Batch 4:  loss: 0.4325993061065674, validation_accuracy: 0.5655999779701233\n",
      "Epoch 123, CIFAR-10 Batch 5:  loss: 0.44796428084373474, validation_accuracy: 0.574400007724762\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss: 0.4899795949459076, validation_accuracy: 0.5662000179290771\n",
      "Epoch 124, CIFAR-10 Batch 2:  loss: 0.41298946738243103, validation_accuracy: 0.569599986076355\n",
      "Epoch 124, CIFAR-10 Batch 3:  loss: 0.4403875470161438, validation_accuracy: 0.548799991607666\n",
      "Epoch 124, CIFAR-10 Batch 4:  loss: 0.41522303223609924, validation_accuracy: 0.5630000233650208\n",
      "Epoch 124, CIFAR-10 Batch 5:  loss: 0.4347359240055084, validation_accuracy: 0.5613999962806702\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss: 0.49646344780921936, validation_accuracy: 0.5703999996185303\n",
      "Epoch 125, CIFAR-10 Batch 2:  loss: 0.41395196318626404, validation_accuracy: 0.5727999806404114\n",
      "Epoch 125, CIFAR-10 Batch 3:  loss: 0.4417075216770172, validation_accuracy: 0.5601999759674072\n",
      "Epoch 125, CIFAR-10 Batch 4:  loss: 0.44540491700172424, validation_accuracy: 0.5676000118255615\n",
      "Epoch 125, CIFAR-10 Batch 5:  loss: 0.40706562995910645, validation_accuracy: 0.5577999949455261\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss: 0.47693291306495667, validation_accuracy: 0.5630000233650208\n",
      "Epoch 126, CIFAR-10 Batch 2:  loss: 0.38641634583473206, validation_accuracy: 0.5726000070571899\n",
      "Epoch 126, CIFAR-10 Batch 3:  loss: 0.41241344809532166, validation_accuracy: 0.5637999773025513\n",
      "Epoch 126, CIFAR-10 Batch 4:  loss: 0.4235905706882477, validation_accuracy: 0.567799985408783\n",
      "Epoch 126, CIFAR-10 Batch 5:  loss: 0.45267659425735474, validation_accuracy: 0.5766000151634216\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss: 0.4511328339576721, validation_accuracy: 0.5730000138282776\n",
      "Epoch 127, CIFAR-10 Batch 2:  loss: 0.35693174600601196, validation_accuracy: 0.5699999928474426\n",
      "Epoch 127, CIFAR-10 Batch 3:  loss: 0.4317747950553894, validation_accuracy: 0.5759999752044678\n",
      "Epoch 127, CIFAR-10 Batch 4:  loss: 0.4256313741207123, validation_accuracy: 0.5758000016212463\n",
      "Epoch 127, CIFAR-10 Batch 5:  loss: 0.40220946073532104, validation_accuracy: 0.5631999969482422\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss: 0.4785148501396179, validation_accuracy: 0.5690000057220459\n",
      "Epoch 128, CIFAR-10 Batch 2:  loss: 0.38258635997772217, validation_accuracy: 0.5694000124931335\n",
      "Epoch 128, CIFAR-10 Batch 3:  loss: 0.39514896273612976, validation_accuracy: 0.5591999888420105\n",
      "Epoch 128, CIFAR-10 Batch 4:  loss: 0.44187507033348083, validation_accuracy: 0.5745999813079834\n",
      "Epoch 128, CIFAR-10 Batch 5:  loss: 0.38785532116889954, validation_accuracy: 0.5734000205993652\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss: 0.4565427899360657, validation_accuracy: 0.5676000118255615\n",
      "Epoch 129, CIFAR-10 Batch 2:  loss: 0.3758327066898346, validation_accuracy: 0.5730000138282776\n",
      "Epoch 129, CIFAR-10 Batch 3:  loss: 0.4047126770019531, validation_accuracy: 0.5636000037193298\n",
      "Epoch 129, CIFAR-10 Batch 4:  loss: 0.4094666540622711, validation_accuracy: 0.5716000199317932\n",
      "Epoch 129, CIFAR-10 Batch 5:  loss: 0.44261035323143005, validation_accuracy: 0.5691999793052673\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss: 0.4474068582057953, validation_accuracy: 0.5672000050544739\n",
      "Epoch 130, CIFAR-10 Batch 2:  loss: 0.36651018261909485, validation_accuracy: 0.5672000050544739\n",
      "Epoch 130, CIFAR-10 Batch 3:  loss: 0.43920761346817017, validation_accuracy: 0.5583999752998352\n",
      "Epoch 130, CIFAR-10 Batch 4:  loss: 0.4238048493862152, validation_accuracy: 0.5716000199317932\n",
      "Epoch 130, CIFAR-10 Batch 5:  loss: 0.4113900065422058, validation_accuracy: 0.567799985408783\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss: 0.4931880831718445, validation_accuracy: 0.5636000037193298\n",
      "Epoch 131, CIFAR-10 Batch 2:  loss: 0.3750773072242737, validation_accuracy: 0.5636000037193298\n",
      "Epoch 131, CIFAR-10 Batch 3:  loss: 0.41439199447631836, validation_accuracy: 0.5626000165939331\n",
      "Epoch 131, CIFAR-10 Batch 4:  loss: 0.39664211869239807, validation_accuracy: 0.5770000219345093\n",
      "Epoch 131, CIFAR-10 Batch 5:  loss: 0.407571017742157, validation_accuracy: 0.5784000158309937\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss: 0.44141268730163574, validation_accuracy: 0.5709999799728394\n",
      "Epoch 132, CIFAR-10 Batch 2:  loss: 0.3583768904209137, validation_accuracy: 0.5712000131607056\n",
      "Epoch 132, CIFAR-10 Batch 3:  loss: 0.398985892534256, validation_accuracy: 0.5616000294685364\n",
      "Epoch 132, CIFAR-10 Batch 4:  loss: 0.41691550612449646, validation_accuracy: 0.569599986076355\n",
      "Epoch 132, CIFAR-10 Batch 5:  loss: 0.3969237208366394, validation_accuracy: 0.5669999718666077\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss: 0.44826045632362366, validation_accuracy: 0.573199987411499\n",
      "Epoch 133, CIFAR-10 Batch 2:  loss: 0.37243932485580444, validation_accuracy: 0.5637999773025513\n",
      "Epoch 133, CIFAR-10 Batch 3:  loss: 0.40955018997192383, validation_accuracy: 0.5569999814033508\n",
      "Epoch 133, CIFAR-10 Batch 4:  loss: 0.412590891122818, validation_accuracy: 0.5627999901771545\n",
      "Epoch 133, CIFAR-10 Batch 5:  loss: 0.40958505868911743, validation_accuracy: 0.5681999921798706\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss: 0.4453910291194916, validation_accuracy: 0.5586000084877014\n",
      "Epoch 134, CIFAR-10 Batch 2:  loss: 0.43107643723487854, validation_accuracy: 0.5583999752998352\n",
      "Epoch 134, CIFAR-10 Batch 3:  loss: 0.3711288869380951, validation_accuracy: 0.5600000023841858\n",
      "Epoch 134, CIFAR-10 Batch 4:  loss: 0.41221219301223755, validation_accuracy: 0.5698000192642212\n",
      "Epoch 134, CIFAR-10 Batch 5:  loss: 0.39191532135009766, validation_accuracy: 0.5753999948501587\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss: 0.45086774230003357, validation_accuracy: 0.5636000037193298\n",
      "Epoch 135, CIFAR-10 Batch 2:  loss: 0.37879958748817444, validation_accuracy: 0.5655999779701233\n",
      "Epoch 135, CIFAR-10 Batch 3:  loss: 0.3864296078681946, validation_accuracy: 0.5687999725341797\n",
      "Epoch 135, CIFAR-10 Batch 4:  loss: 0.39689546823501587, validation_accuracy: 0.5667999982833862\n",
      "Epoch 135, CIFAR-10 Batch 5:  loss: 0.3811495900154114, validation_accuracy: 0.5781999826431274\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss: 0.4627322256565094, validation_accuracy: 0.5591999888420105\n",
      "Epoch 136, CIFAR-10 Batch 2:  loss: 0.3761616349220276, validation_accuracy: 0.5544000267982483\n",
      "Epoch 136, CIFAR-10 Batch 3:  loss: 0.41525593400001526, validation_accuracy: 0.5429999828338623\n",
      "Epoch 136, CIFAR-10 Batch 4:  loss: 0.40149563550949097, validation_accuracy: 0.567799985408783\n",
      "Epoch 136, CIFAR-10 Batch 5:  loss: 0.41433924436569214, validation_accuracy: 0.5740000009536743\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss: 0.4558680057525635, validation_accuracy: 0.5654000043869019\n",
      "Epoch 137, CIFAR-10 Batch 2:  loss: 0.3385113775730133, validation_accuracy: 0.5640000104904175\n",
      "Epoch 137, CIFAR-10 Batch 3:  loss: 0.3447439670562744, validation_accuracy: 0.5651999711990356\n",
      "Epoch 137, CIFAR-10 Batch 4:  loss: 0.3642613887786865, validation_accuracy: 0.5767999887466431\n",
      "Epoch 137, CIFAR-10 Batch 5:  loss: 0.3814826011657715, validation_accuracy: 0.5788000226020813\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss: 0.4200875163078308, validation_accuracy: 0.5583999752998352\n",
      "Epoch 138, CIFAR-10 Batch 2:  loss: 0.3418125510215759, validation_accuracy: 0.5705999732017517\n",
      "Epoch 138, CIFAR-10 Batch 3:  loss: 0.3295952379703522, validation_accuracy: 0.5645999908447266\n",
      "Epoch 138, CIFAR-10 Batch 4:  loss: 0.3554229140281677, validation_accuracy: 0.5770000219345093\n",
      "Epoch 138, CIFAR-10 Batch 5:  loss: 0.38373905420303345, validation_accuracy: 0.5763999819755554\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss: 0.46615052223205566, validation_accuracy: 0.5511999726295471\n",
      "Epoch 139, CIFAR-10 Batch 2:  loss: 0.3454805314540863, validation_accuracy: 0.569599986076355\n",
      "Epoch 139, CIFAR-10 Batch 3:  loss: 0.32615265250205994, validation_accuracy: 0.5641999840736389\n",
      "Epoch 139, CIFAR-10 Batch 4:  loss: 0.4031674861907959, validation_accuracy: 0.5716000199317932\n",
      "Epoch 139, CIFAR-10 Batch 5:  loss: 0.4150388836860657, validation_accuracy: 0.5705999732017517\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss: 0.43541890382766724, validation_accuracy: 0.5532000064849854\n",
      "Epoch 140, CIFAR-10 Batch 2:  loss: 0.33143171668052673, validation_accuracy: 0.5604000091552734\n",
      "Epoch 140, CIFAR-10 Batch 3:  loss: 0.323320209980011, validation_accuracy: 0.5637999773025513\n",
      "Epoch 140, CIFAR-10 Batch 4:  loss: 0.36416423320770264, validation_accuracy: 0.5799999833106995\n",
      "Epoch 140, CIFAR-10 Batch 5:  loss: 0.43908604979515076, validation_accuracy: 0.5691999793052673\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss: 0.4410415291786194, validation_accuracy: 0.5514000058174133\n",
      "Epoch 141, CIFAR-10 Batch 2:  loss: 0.336264431476593, validation_accuracy: 0.5699999928474426\n",
      "Epoch 141, CIFAR-10 Batch 3:  loss: 0.3186187148094177, validation_accuracy: 0.5734000205993652\n",
      "Epoch 141, CIFAR-10 Batch 4:  loss: 0.38060179352760315, validation_accuracy: 0.5726000070571899\n",
      "Epoch 141, CIFAR-10 Batch 5:  loss: 0.41848254203796387, validation_accuracy: 0.5767999887466431\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss: 0.43079718947410583, validation_accuracy: 0.5631999969482422\n",
      "Epoch 142, CIFAR-10 Batch 2:  loss: 0.3135599195957184, validation_accuracy: 0.5672000050544739\n",
      "Epoch 142, CIFAR-10 Batch 3:  loss: 0.33782199025154114, validation_accuracy: 0.5605999827384949\n",
      "Epoch 142, CIFAR-10 Batch 4:  loss: 0.3926052749156952, validation_accuracy: 0.5803999900817871\n",
      "Epoch 142, CIFAR-10 Batch 5:  loss: 0.38345107436180115, validation_accuracy: 0.579800009727478\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss: 0.4129464030265808, validation_accuracy: 0.5604000091552734\n",
      "Epoch 143, CIFAR-10 Batch 2:  loss: 0.3140462338924408, validation_accuracy: 0.5527999997138977\n",
      "Epoch 143, CIFAR-10 Batch 3:  loss: 0.3236992061138153, validation_accuracy: 0.5730000138282776\n",
      "Epoch 143, CIFAR-10 Batch 4:  loss: 0.36030930280685425, validation_accuracy: 0.5741999745368958\n",
      "Epoch 143, CIFAR-10 Batch 5:  loss: 0.3636051118373871, validation_accuracy: 0.5799999833106995\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss: 0.4048524796962738, validation_accuracy: 0.5637999773025513\n",
      "Epoch 144, CIFAR-10 Batch 2:  loss: 0.29928725957870483, validation_accuracy: 0.5633999705314636\n",
      "Epoch 144, CIFAR-10 Batch 3:  loss: 0.3211333751678467, validation_accuracy: 0.5658000111579895\n",
      "Epoch 144, CIFAR-10 Batch 4:  loss: 0.38881751894950867, validation_accuracy: 0.5663999915122986\n",
      "Epoch 144, CIFAR-10 Batch 5:  loss: 0.37274131178855896, validation_accuracy: 0.5789999961853027\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss: 0.4055246412754059, validation_accuracy: 0.5497999787330627\n",
      "Epoch 145, CIFAR-10 Batch 2:  loss: 0.3361286520957947, validation_accuracy: 0.5680000185966492\n",
      "Epoch 145, CIFAR-10 Batch 3:  loss: 0.3225786089897156, validation_accuracy: 0.5758000016212463\n",
      "Epoch 145, CIFAR-10 Batch 4:  loss: 0.41734495759010315, validation_accuracy: 0.5659999847412109\n",
      "Epoch 145, CIFAR-10 Batch 5:  loss: 0.3728974461555481, validation_accuracy: 0.5774000287055969\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss: 0.4071100354194641, validation_accuracy: 0.5622000098228455\n",
      "Epoch 146, CIFAR-10 Batch 2:  loss: 0.31569188833236694, validation_accuracy: 0.5685999989509583\n",
      "Epoch 146, CIFAR-10 Batch 3:  loss: 0.3354014456272125, validation_accuracy: 0.5741999745368958\n",
      "Epoch 146, CIFAR-10 Batch 4:  loss: 0.3642818331718445, validation_accuracy: 0.5771999955177307\n",
      "Epoch 146, CIFAR-10 Batch 5:  loss: 0.3415709137916565, validation_accuracy: 0.5734000205993652\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss: 0.39991509914398193, validation_accuracy: 0.5651999711990356\n",
      "Epoch 147, CIFAR-10 Batch 2:  loss: 0.30613306164741516, validation_accuracy: 0.557200014591217\n",
      "Epoch 147, CIFAR-10 Batch 3:  loss: 0.3292865753173828, validation_accuracy: 0.5564000010490417\n",
      "Epoch 147, CIFAR-10 Batch 4:  loss: 0.3452114164829254, validation_accuracy: 0.5803999900817871\n",
      "Epoch 147, CIFAR-10 Batch 5:  loss: 0.3565773367881775, validation_accuracy: 0.5716000199317932\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss: 0.42122358083724976, validation_accuracy: 0.5612000226974487\n",
      "Epoch 148, CIFAR-10 Batch 2:  loss: 0.3082791268825531, validation_accuracy: 0.5590000152587891\n",
      "Epoch 148, CIFAR-10 Batch 3:  loss: 0.33484217524528503, validation_accuracy: 0.5630000233650208\n",
      "Epoch 148, CIFAR-10 Batch 4:  loss: 0.36555424332618713, validation_accuracy: 0.5666000247001648\n",
      "Epoch 148, CIFAR-10 Batch 5:  loss: 0.36234337091445923, validation_accuracy: 0.5759999752044678\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss: 0.41818007826805115, validation_accuracy: 0.5667999982833862\n",
      "Epoch 149, CIFAR-10 Batch 2:  loss: 0.2978564202785492, validation_accuracy: 0.5595999956130981\n",
      "Epoch 149, CIFAR-10 Batch 3:  loss: 0.37940138578414917, validation_accuracy: 0.5631999969482422\n",
      "Epoch 149, CIFAR-10 Batch 4:  loss: 0.3685661554336548, validation_accuracy: 0.5717999935150146\n",
      "Epoch 149, CIFAR-10 Batch 5:  loss: 0.3667447566986084, validation_accuracy: 0.5681999921798706\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss: 0.4140298366546631, validation_accuracy: 0.5735999941825867\n",
      "Epoch 150, CIFAR-10 Batch 2:  loss: 0.28457167744636536, validation_accuracy: 0.5564000010490417\n",
      "Epoch 150, CIFAR-10 Batch 3:  loss: 0.38395780324935913, validation_accuracy: 0.5637999773025513\n",
      "Epoch 150, CIFAR-10 Batch 4:  loss: 0.3739890456199646, validation_accuracy: 0.5676000118255615\n",
      "Epoch 150, CIFAR-10 Batch 5:  loss: 0.3427124619483948, validation_accuracy: 0.5795999765396118\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss: 0.3819214701652527, validation_accuracy: 0.569599986076355\n",
      "Epoch 151, CIFAR-10 Batch 2:  loss: 0.3690612018108368, validation_accuracy: 0.5741999745368958\n",
      "Epoch 151, CIFAR-10 Batch 3:  loss: 0.349854052066803, validation_accuracy: 0.5694000124931335\n",
      "Epoch 151, CIFAR-10 Batch 4:  loss: 0.33276790380477905, validation_accuracy: 0.5723999738693237\n",
      "Epoch 151, CIFAR-10 Batch 5:  loss: 0.34391075372695923, validation_accuracy: 0.5748000144958496\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss: 0.37478142976760864, validation_accuracy: 0.578000009059906\n",
      "Epoch 152, CIFAR-10 Batch 2:  loss: 0.3613576591014862, validation_accuracy: 0.5716000199317932\n",
      "Epoch 152, CIFAR-10 Batch 3:  loss: 0.32427507638931274, validation_accuracy: 0.5684000253677368\n",
      "Epoch 152, CIFAR-10 Batch 4:  loss: 0.32561978697776794, validation_accuracy: 0.5640000104904175\n",
      "Epoch 152, CIFAR-10 Batch 5:  loss: 0.35832467675209045, validation_accuracy: 0.5789999961853027\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss: 0.3871639370918274, validation_accuracy: 0.5698000192642212\n",
      "Epoch 153, CIFAR-10 Batch 2:  loss: 0.3623436391353607, validation_accuracy: 0.5667999982833862\n",
      "Epoch 153, CIFAR-10 Batch 3:  loss: 0.3257156014442444, validation_accuracy: 0.5680000185966492\n",
      "Epoch 153, CIFAR-10 Batch 4:  loss: 0.2890493869781494, validation_accuracy: 0.5759999752044678\n",
      "Epoch 153, CIFAR-10 Batch 5:  loss: 0.3298972249031067, validation_accuracy: 0.5794000029563904\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss: 0.39239686727523804, validation_accuracy: 0.5626000165939331\n",
      "Epoch 154, CIFAR-10 Batch 2:  loss: 0.3240697979927063, validation_accuracy: 0.5662000179290771\n",
      "Epoch 154, CIFAR-10 Batch 3:  loss: 0.3614743649959564, validation_accuracy: 0.5666000247001648\n",
      "Epoch 154, CIFAR-10 Batch 4:  loss: 0.3495689332485199, validation_accuracy: 0.5662000179290771\n",
      "Epoch 154, CIFAR-10 Batch 5:  loss: 0.31287258863449097, validation_accuracy: 0.5753999948501587\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss: 0.37908026576042175, validation_accuracy: 0.5741999745368958\n",
      "Epoch 155, CIFAR-10 Batch 2:  loss: 0.3071569502353668, validation_accuracy: 0.5781999826431274\n",
      "Epoch 155, CIFAR-10 Batch 3:  loss: 0.47644466161727905, validation_accuracy: 0.5681999921798706\n",
      "Epoch 155, CIFAR-10 Batch 4:  loss: 0.2843491733074188, validation_accuracy: 0.576200008392334\n",
      "Epoch 155, CIFAR-10 Batch 5:  loss: 0.3180679380893707, validation_accuracy: 0.5748000144958496\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss: 0.3795117735862732, validation_accuracy: 0.5645999908447266\n",
      "Epoch 156, CIFAR-10 Batch 2:  loss: 0.30335670709609985, validation_accuracy: 0.5601999759674072\n",
      "Epoch 156, CIFAR-10 Batch 3:  loss: 0.42409276962280273, validation_accuracy: 0.5667999982833862\n",
      "Epoch 156, CIFAR-10 Batch 4:  loss: 0.3060340881347656, validation_accuracy: 0.571399986743927\n",
      "Epoch 156, CIFAR-10 Batch 5:  loss: 0.3007175922393799, validation_accuracy: 0.5667999982833862\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss: 0.4026389718055725, validation_accuracy: 0.5645999908447266\n",
      "Epoch 157, CIFAR-10 Batch 2:  loss: 0.29176005721092224, validation_accuracy: 0.5753999948501587\n",
      "Epoch 157, CIFAR-10 Batch 3:  loss: 0.3972941040992737, validation_accuracy: 0.5681999921798706\n",
      "Epoch 157, CIFAR-10 Batch 4:  loss: 0.3095940053462982, validation_accuracy: 0.5720000267028809\n",
      "Epoch 157, CIFAR-10 Batch 5:  loss: 0.3413007855415344, validation_accuracy: 0.5608000159263611\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss: 0.38552507758140564, validation_accuracy: 0.5702000260353088\n",
      "Epoch 158, CIFAR-10 Batch 2:  loss: 0.31582385301589966, validation_accuracy: 0.5522000193595886\n",
      "Epoch 158, CIFAR-10 Batch 3:  loss: 0.3888869881629944, validation_accuracy: 0.5631999969482422\n",
      "Epoch 158, CIFAR-10 Batch 4:  loss: 0.3093629479408264, validation_accuracy: 0.5709999799728394\n",
      "Epoch 158, CIFAR-10 Batch 5:  loss: 0.33128705620765686, validation_accuracy: 0.5745999813079834\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss: 0.36608248949050903, validation_accuracy: 0.5649999976158142\n",
      "Epoch 159, CIFAR-10 Batch 2:  loss: 0.2755318284034729, validation_accuracy: 0.5631999969482422\n",
      "Epoch 159, CIFAR-10 Batch 3:  loss: 0.3536387085914612, validation_accuracy: 0.5559999942779541\n",
      "Epoch 159, CIFAR-10 Batch 4:  loss: 0.31250062584877014, validation_accuracy: 0.5564000010490417\n",
      "Epoch 159, CIFAR-10 Batch 5:  loss: 0.34728503227233887, validation_accuracy: 0.5690000057220459\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss: 0.37050384283065796, validation_accuracy: 0.5648000240325928\n",
      "Epoch 160, CIFAR-10 Batch 2:  loss: 0.373988538980484, validation_accuracy: 0.5730000138282776\n",
      "Epoch 160, CIFAR-10 Batch 3:  loss: 0.36719661951065063, validation_accuracy: 0.5672000050544739\n",
      "Epoch 160, CIFAR-10 Batch 4:  loss: 0.33408522605895996, validation_accuracy: 0.5586000084877014\n",
      "Epoch 160, CIFAR-10 Batch 5:  loss: 0.37644314765930176, validation_accuracy: 0.5608000159263611\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss: 0.3830263018608093, validation_accuracy: 0.5698000192642212\n",
      "Epoch 161, CIFAR-10 Batch 2:  loss: 0.3296786844730377, validation_accuracy: 0.5612000226974487\n",
      "Epoch 161, CIFAR-10 Batch 3:  loss: 0.29145604372024536, validation_accuracy: 0.5586000084877014\n",
      "Epoch 161, CIFAR-10 Batch 4:  loss: 0.3171847462654114, validation_accuracy: 0.5622000098228455\n",
      "Epoch 161, CIFAR-10 Batch 5:  loss: 0.383279025554657, validation_accuracy: 0.5630000233650208\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss: 0.3731204569339752, validation_accuracy: 0.5626000165939331\n",
      "Epoch 162, CIFAR-10 Batch 2:  loss: 0.35290902853012085, validation_accuracy: 0.5651999711990356\n",
      "Epoch 162, CIFAR-10 Batch 3:  loss: 0.3024516701698303, validation_accuracy: 0.5662000179290771\n",
      "Epoch 162, CIFAR-10 Batch 4:  loss: 0.2993853986263275, validation_accuracy: 0.5616000294685364\n",
      "Epoch 162, CIFAR-10 Batch 5:  loss: 0.3541325628757477, validation_accuracy: 0.5663999915122986\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss: 0.3937475085258484, validation_accuracy: 0.5709999799728394\n",
      "Epoch 163, CIFAR-10 Batch 2:  loss: 0.2883065640926361, validation_accuracy: 0.5669999718666077\n",
      "Epoch 163, CIFAR-10 Batch 3:  loss: 0.2868930697441101, validation_accuracy: 0.5594000220298767\n",
      "Epoch 163, CIFAR-10 Batch 4:  loss: 0.2910084128379822, validation_accuracy: 0.5626000165939331\n",
      "Epoch 163, CIFAR-10 Batch 5:  loss: 0.32019147276878357, validation_accuracy: 0.5690000057220459\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss: 0.3752647042274475, validation_accuracy: 0.5608000159263611\n",
      "Epoch 164, CIFAR-10 Batch 2:  loss: 0.2921122610569, validation_accuracy: 0.569599986076355\n",
      "Epoch 164, CIFAR-10 Batch 3:  loss: 0.2962619662284851, validation_accuracy: 0.5712000131607056\n",
      "Epoch 164, CIFAR-10 Batch 4:  loss: 0.29638129472732544, validation_accuracy: 0.5580000281333923\n",
      "Epoch 164, CIFAR-10 Batch 5:  loss: 0.3300895392894745, validation_accuracy: 0.5618000030517578\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss: 0.3739560842514038, validation_accuracy: 0.5663999915122986\n",
      "Epoch 165, CIFAR-10 Batch 2:  loss: 0.27303624153137207, validation_accuracy: 0.569599986076355\n",
      "Epoch 165, CIFAR-10 Batch 3:  loss: 0.27725571393966675, validation_accuracy: 0.5659999847412109\n",
      "Epoch 165, CIFAR-10 Batch 4:  loss: 0.3527638912200928, validation_accuracy: 0.5551999807357788\n",
      "Epoch 165, CIFAR-10 Batch 5:  loss: 0.3396167457103729, validation_accuracy: 0.5559999942779541\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss: 0.35459649562835693, validation_accuracy: 0.5687999725341797\n",
      "Epoch 166, CIFAR-10 Batch 2:  loss: 0.2740669846534729, validation_accuracy: 0.5740000009536743\n",
      "Epoch 166, CIFAR-10 Batch 3:  loss: 0.31781134009361267, validation_accuracy: 0.5636000037193298\n",
      "Epoch 166, CIFAR-10 Batch 4:  loss: 0.3235660195350647, validation_accuracy: 0.5550000071525574\n",
      "Epoch 166, CIFAR-10 Batch 5:  loss: 0.36370688676834106, validation_accuracy: 0.5637999773025513\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss: 0.37941503524780273, validation_accuracy: 0.5641999840736389\n",
      "Epoch 167, CIFAR-10 Batch 2:  loss: 0.2914654314517975, validation_accuracy: 0.5712000131607056\n",
      "Epoch 167, CIFAR-10 Batch 3:  loss: 0.30839815735816956, validation_accuracy: 0.569599986076355\n",
      "Epoch 167, CIFAR-10 Batch 4:  loss: 0.3333064019680023, validation_accuracy: 0.5648000240325928\n",
      "Epoch 167, CIFAR-10 Batch 5:  loss: 0.3404809832572937, validation_accuracy: 0.5658000111579895\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss: 0.36413177847862244, validation_accuracy: 0.574999988079071\n",
      "Epoch 168, CIFAR-10 Batch 2:  loss: 0.2667316794395447, validation_accuracy: 0.5723999738693237\n",
      "Epoch 168, CIFAR-10 Batch 3:  loss: 0.28440800309181213, validation_accuracy: 0.5709999799728394\n",
      "Epoch 168, CIFAR-10 Batch 4:  loss: 0.2963792085647583, validation_accuracy: 0.5655999779701233\n",
      "Epoch 168, CIFAR-10 Batch 5:  loss: 0.34997767210006714, validation_accuracy: 0.5586000084877014\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss: 0.3568042814731598, validation_accuracy: 0.5673999786376953\n",
      "Epoch 169, CIFAR-10 Batch 2:  loss: 0.3011660575866699, validation_accuracy: 0.5756000280380249\n",
      "Epoch 169, CIFAR-10 Batch 3:  loss: 0.28708615899086, validation_accuracy: 0.5774000287055969\n",
      "Epoch 169, CIFAR-10 Batch 4:  loss: 0.29915088415145874, validation_accuracy: 0.5623999834060669\n",
      "Epoch 169, CIFAR-10 Batch 5:  loss: 0.3271108865737915, validation_accuracy: 0.5604000091552734\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss: 0.3489009439945221, validation_accuracy: 0.5720000267028809\n",
      "Epoch 170, CIFAR-10 Batch 2:  loss: 0.2881079316139221, validation_accuracy: 0.5741999745368958\n",
      "Epoch 170, CIFAR-10 Batch 3:  loss: 0.2931879162788391, validation_accuracy: 0.5680000185966492\n",
      "Epoch 170, CIFAR-10 Batch 4:  loss: 0.3048431873321533, validation_accuracy: 0.5612000226974487\n",
      "Epoch 170, CIFAR-10 Batch 5:  loss: 0.30889296531677246, validation_accuracy: 0.5658000111579895\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss: 0.36473435163497925, validation_accuracy: 0.5722000002861023\n",
      "Epoch 171, CIFAR-10 Batch 2:  loss: 0.28576910495758057, validation_accuracy: 0.574999988079071\n",
      "Epoch 171, CIFAR-10 Batch 3:  loss: 0.27642470598220825, validation_accuracy: 0.5756000280380249\n",
      "Epoch 171, CIFAR-10 Batch 4:  loss: 0.30420953035354614, validation_accuracy: 0.5601999759674072\n",
      "Epoch 171, CIFAR-10 Batch 5:  loss: 0.3072475790977478, validation_accuracy: 0.5609999895095825\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss: 0.3400220572948456, validation_accuracy: 0.5691999793052673\n",
      "Epoch 172, CIFAR-10 Batch 2:  loss: 0.29698583483695984, validation_accuracy: 0.5771999955177307\n",
      "Epoch 172, CIFAR-10 Batch 3:  loss: 0.29244258999824524, validation_accuracy: 0.576200008392334\n",
      "Epoch 172, CIFAR-10 Batch 4:  loss: 0.27188020944595337, validation_accuracy: 0.5627999901771545\n",
      "Epoch 172, CIFAR-10 Batch 5:  loss: 0.3066999614238739, validation_accuracy: 0.5658000111579895\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss: 0.3608011305332184, validation_accuracy: 0.5756000280380249\n",
      "Epoch 173, CIFAR-10 Batch 2:  loss: 0.2857806086540222, validation_accuracy: 0.5738000273704529\n",
      "Epoch 173, CIFAR-10 Batch 3:  loss: 0.3007868528366089, validation_accuracy: 0.5784000158309937\n",
      "Epoch 173, CIFAR-10 Batch 4:  loss: 0.28260019421577454, validation_accuracy: 0.5600000023841858\n",
      "Epoch 173, CIFAR-10 Batch 5:  loss: 0.31919431686401367, validation_accuracy: 0.5722000002861023\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss: 0.34646037220954895, validation_accuracy: 0.5766000151634216\n",
      "Epoch 174, CIFAR-10 Batch 2:  loss: 0.2590197026729584, validation_accuracy: 0.5716000199317932\n",
      "Epoch 174, CIFAR-10 Batch 3:  loss: 0.24960622191429138, validation_accuracy: 0.5708000063896179\n",
      "Epoch 174, CIFAR-10 Batch 4:  loss: 0.2922136187553406, validation_accuracy: 0.5565999746322632\n",
      "Epoch 174, CIFAR-10 Batch 5:  loss: 0.31411075592041016, validation_accuracy: 0.5730000138282776\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss: 0.32418081164360046, validation_accuracy: 0.5699999928474426\n",
      "Epoch 175, CIFAR-10 Batch 2:  loss: 0.2727378308773041, validation_accuracy: 0.5781999826431274\n",
      "Epoch 175, CIFAR-10 Batch 3:  loss: 0.25496748089790344, validation_accuracy: 0.5699999928474426\n",
      "Epoch 175, CIFAR-10 Batch 4:  loss: 0.2682728171348572, validation_accuracy: 0.5666000247001648\n",
      "Epoch 175, CIFAR-10 Batch 5:  loss: 0.29550713300704956, validation_accuracy: 0.5559999942779541\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss: 0.36951765418052673, validation_accuracy: 0.5708000063896179\n",
      "Epoch 176, CIFAR-10 Batch 2:  loss: 0.2681359052658081, validation_accuracy: 0.5640000104904175\n",
      "Epoch 176, CIFAR-10 Batch 3:  loss: 0.24175682663917542, validation_accuracy: 0.5712000131607056\n",
      "Epoch 176, CIFAR-10 Batch 4:  loss: 0.24869632720947266, validation_accuracy: 0.5605999827384949\n",
      "Epoch 176, CIFAR-10 Batch 5:  loss: 0.29775553941726685, validation_accuracy: 0.5633999705314636\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss: 0.327728271484375, validation_accuracy: 0.573199987411499\n",
      "Epoch 177, CIFAR-10 Batch 2:  loss: 0.2603525221347809, validation_accuracy: 0.5767999887466431\n",
      "Epoch 177, CIFAR-10 Batch 3:  loss: 0.2569034695625305, validation_accuracy: 0.5694000124931335\n",
      "Epoch 177, CIFAR-10 Batch 4:  loss: 0.2651100754737854, validation_accuracy: 0.5522000193595886\n",
      "Epoch 177, CIFAR-10 Batch 5:  loss: 0.2867273688316345, validation_accuracy: 0.557200014591217\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss: 0.3497978448867798, validation_accuracy: 0.5702000260353088\n",
      "Epoch 178, CIFAR-10 Batch 2:  loss: 0.2700100541114807, validation_accuracy: 0.5684000253677368\n",
      "Epoch 178, CIFAR-10 Batch 3:  loss: 0.269186794757843, validation_accuracy: 0.5684000253677368\n",
      "Epoch 178, CIFAR-10 Batch 4:  loss: 0.2599683701992035, validation_accuracy: 0.5630000233650208\n",
      "Epoch 178, CIFAR-10 Batch 5:  loss: 0.27840712666511536, validation_accuracy: 0.5550000071525574\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss: 0.3298030495643616, validation_accuracy: 0.5691999793052673\n",
      "Epoch 179, CIFAR-10 Batch 2:  loss: 0.27527326345443726, validation_accuracy: 0.5730000138282776\n",
      "Epoch 179, CIFAR-10 Batch 3:  loss: 0.289806067943573, validation_accuracy: 0.5667999982833862\n",
      "Epoch 179, CIFAR-10 Batch 4:  loss: 0.2722959518432617, validation_accuracy: 0.569599986076355\n",
      "Epoch 179, CIFAR-10 Batch 5:  loss: 0.2945542335510254, validation_accuracy: 0.5691999793052673\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss: 0.3286725580692291, validation_accuracy: 0.5694000124931335\n",
      "Epoch 180, CIFAR-10 Batch 2:  loss: 0.26829665899276733, validation_accuracy: 0.5716000199317932\n",
      "Epoch 180, CIFAR-10 Batch 3:  loss: 0.23985858261585236, validation_accuracy: 0.573199987411499\n",
      "Epoch 180, CIFAR-10 Batch 4:  loss: 0.23155470192432404, validation_accuracy: 0.5691999793052673\n",
      "Epoch 180, CIFAR-10 Batch 5:  loss: 0.258007675409317, validation_accuracy: 0.5649999976158142\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss: 0.338287889957428, validation_accuracy: 0.5723999738693237\n",
      "Epoch 181, CIFAR-10 Batch 2:  loss: 0.25894099473953247, validation_accuracy: 0.5680000185966492\n",
      "Epoch 181, CIFAR-10 Batch 3:  loss: 0.26381510496139526, validation_accuracy: 0.5685999989509583\n",
      "Epoch 181, CIFAR-10 Batch 4:  loss: 0.26691314578056335, validation_accuracy: 0.5583999752998352\n",
      "Epoch 181, CIFAR-10 Batch 5:  loss: 0.2582566440105438, validation_accuracy: 0.5586000084877014\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss: 0.34506192803382874, validation_accuracy: 0.5730000138282776\n",
      "Epoch 182, CIFAR-10 Batch 2:  loss: 0.26385244727134705, validation_accuracy: 0.5735999941825867\n",
      "Epoch 182, CIFAR-10 Batch 3:  loss: 0.24588081240653992, validation_accuracy: 0.5740000009536743\n",
      "Epoch 182, CIFAR-10 Batch 4:  loss: 0.26422834396362305, validation_accuracy: 0.5527999997138977\n",
      "Epoch 182, CIFAR-10 Batch 5:  loss: 0.28323084115982056, validation_accuracy: 0.5591999888420105\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss: 0.3484811782836914, validation_accuracy: 0.5741999745368958\n",
      "Epoch 183, CIFAR-10 Batch 2:  loss: 0.2627941071987152, validation_accuracy: 0.5623999834060669\n",
      "Epoch 183, CIFAR-10 Batch 3:  loss: 0.26306378841400146, validation_accuracy: 0.5709999799728394\n",
      "Epoch 183, CIFAR-10 Batch 4:  loss: 0.28515154123306274, validation_accuracy: 0.5546000003814697\n",
      "Epoch 183, CIFAR-10 Batch 5:  loss: 0.2633887529373169, validation_accuracy: 0.5577999949455261\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss: 0.33919548988342285, validation_accuracy: 0.5680000185966492\n",
      "Epoch 184, CIFAR-10 Batch 2:  loss: 0.25312238931655884, validation_accuracy: 0.5727999806404114\n",
      "Epoch 184, CIFAR-10 Batch 3:  loss: 0.24299068748950958, validation_accuracy: 0.5730000138282776\n",
      "Epoch 184, CIFAR-10 Batch 4:  loss: 0.21510040760040283, validation_accuracy: 0.5666000247001648\n",
      "Epoch 184, CIFAR-10 Batch 5:  loss: 0.2561286985874176, validation_accuracy: 0.5655999779701233\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss: 0.3268715739250183, validation_accuracy: 0.5712000131607056\n",
      "Epoch 185, CIFAR-10 Batch 2:  loss: 0.2445879876613617, validation_accuracy: 0.5648000240325928\n",
      "Epoch 185, CIFAR-10 Batch 3:  loss: 0.24548156559467316, validation_accuracy: 0.5655999779701233\n",
      "Epoch 185, CIFAR-10 Batch 4:  loss: 0.2584141790866852, validation_accuracy: 0.5514000058174133\n",
      "Epoch 185, CIFAR-10 Batch 5:  loss: 0.27666234970092773, validation_accuracy: 0.5651999711990356\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss: 0.3231745660305023, validation_accuracy: 0.5691999793052673\n",
      "Epoch 186, CIFAR-10 Batch 2:  loss: 0.2340954840183258, validation_accuracy: 0.5752000212669373\n",
      "Epoch 186, CIFAR-10 Batch 3:  loss: 0.2545422613620758, validation_accuracy: 0.5748000144958496\n",
      "Epoch 186, CIFAR-10 Batch 4:  loss: 0.22720496356487274, validation_accuracy: 0.5702000260353088\n",
      "Epoch 186, CIFAR-10 Batch 5:  loss: 0.262563556432724, validation_accuracy: 0.5636000037193298\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss: 0.3262648582458496, validation_accuracy: 0.5691999793052673\n",
      "Epoch 187, CIFAR-10 Batch 2:  loss: 0.254680335521698, validation_accuracy: 0.5720000267028809\n",
      "Epoch 187, CIFAR-10 Batch 3:  loss: 0.270546019077301, validation_accuracy: 0.5663999915122986\n",
      "Epoch 187, CIFAR-10 Batch 4:  loss: 0.22935953736305237, validation_accuracy: 0.5676000118255615\n",
      "Epoch 187, CIFAR-10 Batch 5:  loss: 0.26463016867637634, validation_accuracy: 0.5544000267982483\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss: 0.337481290102005, validation_accuracy: 0.5726000070571899\n",
      "Epoch 188, CIFAR-10 Batch 2:  loss: 0.23728308081626892, validation_accuracy: 0.5649999976158142\n",
      "Epoch 188, CIFAR-10 Batch 3:  loss: 0.2892588675022125, validation_accuracy: 0.5726000070571899\n",
      "Epoch 188, CIFAR-10 Batch 4:  loss: 0.25029081106185913, validation_accuracy: 0.5722000002861023\n",
      "Epoch 188, CIFAR-10 Batch 5:  loss: 0.2857438623905182, validation_accuracy: 0.5631999969482422\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss: 0.3354718089103699, validation_accuracy: 0.5702000260353088\n",
      "Epoch 189, CIFAR-10 Batch 2:  loss: 0.23865003883838654, validation_accuracy: 0.571399986743927\n",
      "Epoch 189, CIFAR-10 Batch 3:  loss: 0.2752242088317871, validation_accuracy: 0.5727999806404114\n",
      "Epoch 189, CIFAR-10 Batch 4:  loss: 0.21631965041160583, validation_accuracy: 0.5626000165939331\n",
      "Epoch 189, CIFAR-10 Batch 5:  loss: 0.2917037606239319, validation_accuracy: 0.5663999915122986\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss: 0.34029150009155273, validation_accuracy: 0.5712000131607056\n",
      "Epoch 190, CIFAR-10 Batch 2:  loss: 0.24894146621227264, validation_accuracy: 0.5648000240325928\n",
      "Epoch 190, CIFAR-10 Batch 3:  loss: 0.21914903819561005, validation_accuracy: 0.5609999895095825\n",
      "Epoch 190, CIFAR-10 Batch 4:  loss: 0.24020858108997345, validation_accuracy: 0.5609999895095825\n",
      "Epoch 190, CIFAR-10 Batch 5:  loss: 0.2533259987831116, validation_accuracy: 0.5623999834060669\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss: 0.3567226827144623, validation_accuracy: 0.5777999758720398\n",
      "Epoch 191, CIFAR-10 Batch 2:  loss: 0.24145767092704773, validation_accuracy: 0.573199987411499\n",
      "Epoch 191, CIFAR-10 Batch 3:  loss: 0.23971323668956757, validation_accuracy: 0.5691999793052673\n",
      "Epoch 191, CIFAR-10 Batch 4:  loss: 0.21457087993621826, validation_accuracy: 0.5618000030517578\n",
      "Epoch 191, CIFAR-10 Batch 5:  loss: 0.21329717338085175, validation_accuracy: 0.5691999793052673\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss: 0.3375493288040161, validation_accuracy: 0.5690000057220459\n",
      "Epoch 192, CIFAR-10 Batch 2:  loss: 0.21791324019432068, validation_accuracy: 0.5741999745368958\n",
      "Epoch 192, CIFAR-10 Batch 3:  loss: 0.21954575181007385, validation_accuracy: 0.5748000144958496\n",
      "Epoch 192, CIFAR-10 Batch 4:  loss: 0.24638716876506805, validation_accuracy: 0.5598000288009644\n",
      "Epoch 192, CIFAR-10 Batch 5:  loss: 0.22981305420398712, validation_accuracy: 0.5663999915122986\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss: 0.3235355019569397, validation_accuracy: 0.5727999806404114\n",
      "Epoch 193, CIFAR-10 Batch 2:  loss: 0.22162044048309326, validation_accuracy: 0.574999988079071\n",
      "Epoch 193, CIFAR-10 Batch 3:  loss: 0.23714382946491241, validation_accuracy: 0.5663999915122986\n",
      "Epoch 193, CIFAR-10 Batch 4:  loss: 0.21427130699157715, validation_accuracy: 0.5640000104904175\n",
      "Epoch 193, CIFAR-10 Batch 5:  loss: 0.22269988059997559, validation_accuracy: 0.5659999847412109\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss: 0.32894590497016907, validation_accuracy: 0.5654000043869019\n",
      "Epoch 194, CIFAR-10 Batch 2:  loss: 0.2507639527320862, validation_accuracy: 0.5809999704360962\n",
      "Epoch 194, CIFAR-10 Batch 3:  loss: 0.2604139745235443, validation_accuracy: 0.5645999908447266\n",
      "Epoch 194, CIFAR-10 Batch 4:  loss: 0.23943233489990234, validation_accuracy: 0.5580000281333923\n",
      "Epoch 194, CIFAR-10 Batch 5:  loss: 0.2334902584552765, validation_accuracy: 0.579200029373169\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss: 0.3072514235973358, validation_accuracy: 0.5681999921798706\n",
      "Epoch 195, CIFAR-10 Batch 2:  loss: 0.23602381348609924, validation_accuracy: 0.5669999718666077\n",
      "Epoch 195, CIFAR-10 Batch 3:  loss: 0.25864964723587036, validation_accuracy: 0.5766000151634216\n",
      "Epoch 195, CIFAR-10 Batch 4:  loss: 0.19419491291046143, validation_accuracy: 0.5555999875068665\n",
      "Epoch 195, CIFAR-10 Batch 5:  loss: 0.22958596050739288, validation_accuracy: 0.5699999928474426\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss: 0.31051674485206604, validation_accuracy: 0.5669999718666077\n",
      "Epoch 196, CIFAR-10 Batch 2:  loss: 0.2256346195936203, validation_accuracy: 0.553600013256073\n",
      "Epoch 196, CIFAR-10 Batch 3:  loss: 0.2777628004550934, validation_accuracy: 0.5703999996185303\n",
      "Epoch 196, CIFAR-10 Batch 4:  loss: 0.21425461769104004, validation_accuracy: 0.5347999930381775\n",
      "Epoch 196, CIFAR-10 Batch 5:  loss: 0.23043331503868103, validation_accuracy: 0.5753999948501587\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss: 0.32439368963241577, validation_accuracy: 0.5672000050544739\n",
      "Epoch 197, CIFAR-10 Batch 2:  loss: 0.22674140334129333, validation_accuracy: 0.578000009059906\n",
      "Epoch 197, CIFAR-10 Batch 3:  loss: 0.21019962430000305, validation_accuracy: 0.5680000185966492\n",
      "Epoch 197, CIFAR-10 Batch 4:  loss: 0.2535270154476166, validation_accuracy: 0.5515999794006348\n",
      "Epoch 197, CIFAR-10 Batch 5:  loss: 0.20075905323028564, validation_accuracy: 0.5702000260353088\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss: 0.33866086602211, validation_accuracy: 0.5641999840736389\n",
      "Epoch 198, CIFAR-10 Batch 2:  loss: 0.24812301993370056, validation_accuracy: 0.5618000030517578\n",
      "Epoch 198, CIFAR-10 Batch 3:  loss: 0.21003703773021698, validation_accuracy: 0.569599986076355\n",
      "Epoch 198, CIFAR-10 Batch 4:  loss: 0.26513421535491943, validation_accuracy: 0.5637999773025513\n",
      "Epoch 198, CIFAR-10 Batch 5:  loss: 0.24293379485607147, validation_accuracy: 0.5654000043869019\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss: 0.3374635577201843, validation_accuracy: 0.5684000253677368\n",
      "Epoch 199, CIFAR-10 Batch 2:  loss: 0.21488770842552185, validation_accuracy: 0.5705999732017517\n",
      "Epoch 199, CIFAR-10 Batch 3:  loss: 0.22034497559070587, validation_accuracy: 0.5648000240325928\n",
      "Epoch 199, CIFAR-10 Batch 4:  loss: 0.24326086044311523, validation_accuracy: 0.5604000091552734\n",
      "Epoch 199, CIFAR-10 Batch 5:  loss: 0.20672449469566345, validation_accuracy: 0.5727999806404114\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss: 0.3211944103240967, validation_accuracy: 0.5586000084877014\n",
      "Epoch 200, CIFAR-10 Batch 2:  loss: 0.2330983579158783, validation_accuracy: 0.5547999739646912\n",
      "Epoch 200, CIFAR-10 Batch 3:  loss: 0.20393148064613342, validation_accuracy: 0.5709999799728394\n",
      "Epoch 200, CIFAR-10 Batch 4:  loss: 0.21509483456611633, validation_accuracy: 0.5569999814033508\n",
      "Epoch 200, CIFAR-10 Batch 5:  loss: 0.2103358507156372, validation_accuracy: 0.5698000192642212\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.57265625\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc5fd390>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. That's because there are many more techniques that can be applied to your model and we recemmond that once you are done with this project, you explore!\n",
    "\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
